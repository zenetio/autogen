"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[5553],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>m});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function r(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var p=a.createContext({}),s=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=s(e.components);return a.createElement(p.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,o=e.originalType,p=e.parentName,u=r(e,["components","mdxType","originalType","parentName"]),d=s(n),m=i,h=d["".concat(p,".").concat(m)]||d[m]||c[m]||o;return n?a.createElement(h,l(l({ref:t},u),{},{components:n})):a.createElement(h,l({ref:t},u))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var o=n.length,l=new Array(o);l[0]=d;var r={};for(var p in t)hasOwnProperty.call(t,p)&&(r[p]=t[p]);r.originalType=e,r.mdxType="string"==typeof e?e:i,l[1]=r;for(var s=2;s<o;s++)l[s]=n[s];return a.createElement.apply(null,l)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},7784:(e,t,n)=>{n.r(t),n.d(t,{contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>r,toc:()=>p});var a=n(3117),i=(n(7294),n(3905));const o={},l="Installation",r={unversionedId:"Installation",id:"Installation",isDocsHomePage:!1,title:"Installation",description:"Setup Virtual Environment",source:"@site/docs/Installation.md",sourceDirName:".",slug:"/Installation",permalink:"/autogen/docs/Installation",editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/Installation.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Getting Started",permalink:"/autogen/docs/Getting-Started"},next:{title:"Multi-agent Conversation Framework",permalink:"/autogen/docs/Use-Cases/agent_chat"}},p=[{value:"Setup Virtual Environment",id:"setup-virtual-environment",children:[{value:"Option 1: venv",id:"option-1-venv",children:[],level:3},{value:"Option 2: conda",id:"option-2-conda",children:[],level:3},{value:"Option 3: poetry",id:"option-3-poetry",children:[],level:3}],level:2},{value:"Python",id:"python",children:[{value:"Migration guide to v0.2",id:"migration-guide-to-v02",children:[],level:3},{value:"Optional Dependencies",id:"optional-dependencies",children:[],level:3}],level:2}],s={toc:p};function u(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,a.Z)({},s,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"installation"},"Installation"),(0,i.kt)("h2",{id:"setup-virtual-environment"},"Setup Virtual Environment"),(0,i.kt)("p",null,"When not using a docker container, we recommend using a virtual environment to install AutoGen. This will ensure that the dependencies for AutoGen are isolated from the rest of your system."),(0,i.kt)("h3",{id:"option-1-venv"},"Option 1: venv"),(0,i.kt)("p",null,"You can create a virtual environment with ",(0,i.kt)("inlineCode",{parentName:"p"},"venv")," as below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"python3 -m venv pyautogen\nsource pyautogen/bin/activate\n")),(0,i.kt)("p",null,"The following command will deactivate the current ",(0,i.kt)("inlineCode",{parentName:"p"},"venv")," environment:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"deactivate\n")),(0,i.kt)("h3",{id:"option-2-conda"},"Option 2: conda"),(0,i.kt)("p",null,"Another option is with ",(0,i.kt)("inlineCode",{parentName:"p"},"Conda"),", Conda works better at solving dependency conflicts than pip. You can install it by following ",(0,i.kt)("a",{parentName:"p",href:"https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html"},"this doc"),",\nand then create a virtual environment as below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"conda create -n pyautogen python=3.10  # python 3.10 is recommended as it's stable and not too old\nconda activate pyautogen\n")),(0,i.kt)("p",null,"The following command will deactivate the current ",(0,i.kt)("inlineCode",{parentName:"p"},"conda")," environment:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"conda deactivate\n")),(0,i.kt)("h3",{id:"option-3-poetry"},"Option 3: poetry"),(0,i.kt)("p",null,"Another option is with ",(0,i.kt)("inlineCode",{parentName:"p"},"poetry"),", which is a dependency manager for Python."),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://python-poetry.org/docs/"},"Poetry")," is a tool for dependency management and packaging in Python. It allows you to declare the libraries your project depends on and it will manage (install/update) them for you. Poetry offers a lockfile to ensure repeatable installs, and can build your project for distribution."),(0,i.kt)("p",null,"You can install it by following ",(0,i.kt)("a",{parentName:"p",href:"https://python-poetry.org/docs/#installation"},"this doc"),",\nand then create a virtual environment as below:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"poetry init\npoetry shell\n\npoetry add pyautogen\n")),(0,i.kt)("p",null,"The following command will deactivate the current ",(0,i.kt)("inlineCode",{parentName:"p"},"poetry")," environment:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"exit\n")),(0,i.kt)("p",null,"Now, you're ready to install AutoGen in the virtual environment you've just created."),(0,i.kt)("h2",{id:"python"},"Python"),(0,i.kt)("p",null,"AutoGen requires ",(0,i.kt)("strong",{parentName:"p"},"Python version >= 3.8, < 3.12"),". It can be installed from pip:"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install pyautogen\n")),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pyautogen<0.2")," requires ",(0,i.kt)("inlineCode",{parentName:"p"},"openai<1"),". Starting from pyautogen v0.2, ",(0,i.kt)("inlineCode",{parentName:"p"},"openai>=1")," is required."),(0,i.kt)("h3",{id:"migration-guide-to-v02"},"Migration guide to v0.2"),(0,i.kt)("p",null,"openai v1 is a total rewrite of the library with many breaking changes. For example, the inference requires instantiating a client, instead of using a global class method.\nTherefore, some changes are required for users of ",(0,i.kt)("inlineCode",{parentName:"p"},"pyautogen<0.2"),"."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"api_base")," -> ",(0,i.kt)("inlineCode",{parentName:"li"},"base_url"),", ",(0,i.kt)("inlineCode",{parentName:"li"},"request_timeout")," -> ",(0,i.kt)("inlineCode",{parentName:"li"},"timeout")," in ",(0,i.kt)("inlineCode",{parentName:"li"},"llm_config")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"config_list"),". ",(0,i.kt)("inlineCode",{parentName:"li"},"max_retry_period")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"retry_wait_time")," are deprecated. ",(0,i.kt)("inlineCode",{parentName:"li"},"max_retries")," can be set for each client."),(0,i.kt)("li",{parentName:"ul"},"MathChat is unsupported until it is tested in future release."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"autogen.Completion")," and ",(0,i.kt)("inlineCode",{parentName:"li"},"autogen.ChatCompletion")," are deprecated. The essential functionalities are moved to ",(0,i.kt)("inlineCode",{parentName:"li"},"autogen.OpenAIWrapper"),":")),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-python"},'from autogen import OpenAIWrapper\nclient = OpenAIWrapper(config_list=config_list)\nresponse = client.create(messages=[{"role": "user", "content": "2+2="}])\nprint(client.extract_text_or_completion_object(response))\n')),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Inference parameter tuning and inference logging features are currently unavailable in ",(0,i.kt)("inlineCode",{parentName:"li"},"OpenAIWrapper"),". Logging will be added in a future release.\nInference parameter tuning can be done via ",(0,i.kt)("a",{parentName:"li",href:"https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function"},(0,i.kt)("inlineCode",{parentName:"a"},"flaml.tune")),"."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("inlineCode",{parentName:"li"},"seed")," in autogen is renamed into ",(0,i.kt)("inlineCode",{parentName:"li"},"cache_seed")," to accommodate the newly added ",(0,i.kt)("inlineCode",{parentName:"li"},"seed")," param in openai chat completion api. ",(0,i.kt)("inlineCode",{parentName:"li"},"use_cache")," is removed as a kwarg in ",(0,i.kt)("inlineCode",{parentName:"li"},"OpenAIWrapper.create()")," for being automatically decided by ",(0,i.kt)("inlineCode",{parentName:"li"},"cache_seed"),": int | None. The difference between autogen's ",(0,i.kt)("inlineCode",{parentName:"li"},"cache_seed")," and openai's ",(0,i.kt)("inlineCode",{parentName:"li"},"seed")," is that:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"autogen uses local disk cache to guarantee the exactly same output is produced for the same input and when cache is hit, no openai api call will be made."),(0,i.kt)("li",{parentName:"ul"},"openai's ",(0,i.kt)("inlineCode",{parentName:"li"},"seed")," is a best-effort deterministic sampling with no guarantee of determinism. When using openai's ",(0,i.kt)("inlineCode",{parentName:"li"},"seed")," with ",(0,i.kt)("inlineCode",{parentName:"li"},"cache_seed")," set to None, even for the same input, an openai api call will be made and there is no guarantee for getting exactly the same output.")))),(0,i.kt)("h3",{id:"optional-dependencies"},"Optional Dependencies"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("h4",{parentName:"li",id:"docker"},"docker"))),(0,i.kt)("p",null,"For the best user experience and seamless code execution, we highly recommend using Docker with AutoGen. Docker is a containerization platform that simplifies the setup and execution of your code. Developing in a docker container, such as GitHub Codespace, also makes the development convenient."),(0,i.kt)("p",null,"When running AutoGen out of a docker container, to use docker for code execution, you also need to install the python package ",(0,i.kt)("inlineCode",{parentName:"p"},"docker"),":"),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},"pip install docker\n")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("h4",{parentName:"li",id:"blendsearch"},"blendsearch"))),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pyautogen<0.2")," offers a cost-effective hyperparameter optimization technique ",(0,i.kt)("a",{parentName:"p",href:"https://arxiv.org/abs/2303.04673"},"EcoOptiGen")," for tuning Large Language Models. Please install with the ","[blendsearch]"," option to use it."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'pip install "pyautogen[blendsearch]<0.2"\n')),(0,i.kt)("p",null,"Example notebooks:"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/microsoft/autogen/blob/main/notebook/oai_completion.ipynb"},"Optimize for Code Generation")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/microsoft/autogen/blob/main/notebook/oai_chatgpt_gpt4.ipynb"},"Optimize for Math")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("h4",{parentName:"li",id:"retrievechat"},"retrievechat"))),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pyautogen")," supports retrieval-augmented generation tasks such as question answering and code generation with RAG agents. Please install with the ","[retrievechat]"," option to use it."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'pip install "pyautogen[retrievechat]"\n')),(0,i.kt)("p",null,"RetrieveChat can handle various types of documents. By default, it can process\nplain text and PDF files, including formats such as 'txt', 'json', 'csv', 'tsv',\n'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml' and 'pdf'.\nIf you install ",(0,i.kt)("a",{parentName:"p",href:"https://unstructured-io.github.io/unstructured/installation/full_installation.html"},"unstructured"),"\n(",(0,i.kt)("inlineCode",{parentName:"p"},'pip install "unstructured[all-docs]"'),"), additional document types such as 'docx',\n'doc', 'odt', 'pptx', 'ppt', 'xlsx', 'eml', 'msg', 'epub' will also be supported."),(0,i.kt)("p",null,"You can find a list of all supported document types by using ",(0,i.kt)("inlineCode",{parentName:"p"},"autogen.retrieve_utils.TEXT_FORMATS"),"."),(0,i.kt)("p",null,"Example notebooks:"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb"},"Automated Code Generation and Question Answering with Retrieval Augmented Agents")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb"},"Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)")),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb"},"Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("h4",{parentName:"li",id:"teachableagent"},"TeachableAgent"))),(0,i.kt)("p",null,"To use TeachableAgent, please install AutoGen with the ","[teachable]"," option."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'pip install "pyautogen[teachable]"\n')),(0,i.kt)("p",null,"Example notebook:  ",(0,i.kt)("a",{parentName:"p",href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachability.ipynb"},"Chatting with TeachableAgent")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("h4",{parentName:"li",id:"large-multimodal-model-lmm-agents"},"Large Multimodal Model (LMM) Agents"))),(0,i.kt)("p",null,"We offered Multimodal Conversable Agent and LLaVA Agent. Please install with the ","[lmm]"," option to use it."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'pip install "pyautogen[lmm]"\n')),(0,i.kt)("p",null,"Example notebooks:"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb"},"LLaVA Agent")),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("h4",{parentName:"li",id:"mathchat"},"mathchat"))),(0,i.kt)("p",null,(0,i.kt)("inlineCode",{parentName:"p"},"pyautogen<0.2")," offers an experimental agent for math problem solving. Please install with the ","[mathchat]"," option to use it."),(0,i.kt)("pre",null,(0,i.kt)("code",{parentName:"pre",className:"language-bash"},'pip install "pyautogen[mathchat]<0.2"\n')),(0,i.kt)("p",null,"Example notebooks:"),(0,i.kt)("p",null,(0,i.kt)("a",{parentName:"p",href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_MathChat.ipynb"},"Using MathChat to Solve Math Problems")))}u.isMDXComponent=!0}}]);