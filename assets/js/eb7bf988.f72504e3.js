"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6163],{91018:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var a=t(85893),o=t(11151);const i={custom_edit_url:"https://github.com/microsoft/autogen/edit/main/website/docs/topics/non-openai-models/cloud-bedrock.ipynb",description:"Define and load a custom model",source_notebook:"/website/docs/topics/non-openai-models/cloud-bedrock.ipynb",tags:["custom model"],title:"Amazon Bedrock"},s="Amazon Bedrock",r={id:"topics/non-openai-models/cloud-bedrock",title:"Amazon Bedrock",description:"Define and load a custom model",source:"@site/docs/topics/non-openai-models/cloud-bedrock.mdx",sourceDirName:"topics/non-openai-models",slug:"/topics/non-openai-models/cloud-bedrock",permalink:"/autogen/docs/topics/non-openai-models/cloud-bedrock",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/topics/non-openai-models/cloud-bedrock.ipynb",tags:[{label:"custom model",permalink:"/autogen/docs/tags/custom-model"}],version:"current",frontMatter:{custom_edit_url:"https://github.com/microsoft/autogen/edit/main/website/docs/topics/non-openai-models/cloud-bedrock.ipynb",description:"Define and load a custom model",source_notebook:"/website/docs/topics/non-openai-models/cloud-bedrock.ipynb",tags:["custom model"],title:"Amazon Bedrock"},sidebar:"docsSidebar",previous:{title:"Anthropic Claude",permalink:"/autogen/docs/topics/non-openai-models/cloud-anthropic"},next:{title:"Cohere",permalink:"/autogen/docs/topics/non-openai-models/cloud-cohere"}},c={},l=[{value:"Model features / support",id:"model-features-support",level:2},{value:"Requirements",id:"requirements",level:2},{value:"Pricing",id:"pricing",level:2},{value:"Set the config for Amazon Bedrock",id:"set-the-config-for-amazon-bedrock",level:2},{value:"Two-agent Coding Example",id:"two-agent-coding-example",level:2},{value:"Configuration",id:"configuration",level:3},{value:"Construct Agents",id:"construct-agents",level:3},{value:"Initiate Chat",id:"initiate-chat",level:3},{value:"Tool Call Example",id:"tool-call-example",level:2},{value:"Agents",id:"agents",level:3},{value:"Group Chat Example with Anthropic\u2019s Claude 3 Sonnet, Mistral\u2019s Large 2, and Meta\u2019s Llama 3.1 70B",id:"group-chat-example-with-anthropics-claude-3-sonnet-mistrals-large-2-and-metas-llama-3.1-70b",level:2},{value:"Image classification with Anthropic\u2019s Claude 3 Sonnet",id:"image-classification-with-anthropics-claude-3-sonnet",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"amazon-bedrock",children:"Amazon Bedrock"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://colab.research.google.com/github/microsoft/autogen/blob/main/website/docs/topics/non-openai-models/cloud-bedrock.ipynb",children:(0,a.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,a.jsx)(n.a,{href:"https://github.com/microsoft/autogen/blob/main/website/docs/topics/non-openai-models/cloud-bedrock.ipynb",children:(0,a.jsx)(n.img,{src:"https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github",alt:"Open on GitHub"})})]}),"\n",(0,a.jsx)(n.p,{children:"AutoGen allows you to use Amazon\u2019s generative AI Bedrock service to run\ninference with a number of open-weight models and as well as their own\nmodels."}),"\n",(0,a.jsx)(n.p,{children:"Amazon Bedrock supports models from providers such as Meta, Anthropic,\nCohere, and Mistral."}),"\n",(0,a.jsx)(n.p,{children:"In this notebook, we demonstrate how to use Anthropic\u2019s Sonnet model for\nAgentChat in AutoGen."}),"\n",(0,a.jsx)(n.h2,{id:"model-features-support",children:"Model features / support"}),"\n",(0,a.jsxs)(n.p,{children:["Amazon Bedrock supports a wide range of models, not only for text\ngeneration but also for image classification and generation. Not all\nfeatures are supported by AutoGen or by the Converse API used. Please\nsee ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features",children:"Amazon\u2019s\ndocumentation"}),"\non the features supported by the Converse API."]}),"\n",(0,a.jsx)(n.p,{children:"At this point in time AutoGen supports text generation and image\nclassification (passing images to the LLM)."}),"\n",(0,a.jsxs)(n.p,{children:["It does not, yet, support image generation\n(",(0,a.jsx)(n.a,{href:"https://microsoft.github.io/autogen/docs/contributor-guide/contributing/",children:"contribute"}),")."]}),"\n",(0,a.jsx)(n.h2,{id:"requirements",children:"Requirements"}),"\n",(0,a.jsxs)(n.p,{children:["To use Amazon Bedrock with AutoGen, first you need to install the\n",(0,a.jsx)(n.code,{children:"pyautogen[bedrock]"})," package."]}),"\n",(0,a.jsx)(n.h2,{id:"pricing",children:"Pricing"}),"\n",(0,a.jsx)(n.p,{children:"When we combine the number of models supported and costs being on a\nper-region basis, it\u2019s not feasible to maintain the costs for each\nmodel+region combination within the AutoGen implementation. Therefore,\nit\u2019s recommended that you add the following to your config with cost per\n1,000 input and output tokens, respectively:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'{\n    ...\n    "price": [0.003, 0.015]\n    ...\n}\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Amazon Bedrock pricing is available\n",(0,a.jsx)(n.a,{href:"https://aws.amazon.com/bedrock/pricing/",children:"here"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# If you need to install AutoGen with Amazon Bedrock\n!pip install pyautogen["bedrock"]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"set-the-config-for-amazon-bedrock",children:"Set the config for Amazon Bedrock"}),"\n",(0,a.jsxs)(n.p,{children:["Amazon\u2019s Bedrock does not use the ",(0,a.jsx)(n.code,{children:"api_key"})," as per other cloud inference\nproviders for authentication, instead it uses a number of access, token,\nand profile values. These fields will need to be added to your client\nconfiguration. Please check the Amazon Bedrock documentation to\ndetermine which ones you will need to add."]}),"\n",(0,a.jsx)(n.p,{children:"The available parameters are:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"aws_region (mandatory)"}),"\n",(0,a.jsx)(n.li,{children:"aws_access_key (or environment variable: AWS_ACCESS_KEY)"}),"\n",(0,a.jsx)(n.li,{children:"aws_secret_key (or environment variable: AWS_SECRET_KEY)"}),"\n",(0,a.jsx)(n.li,{children:"aws_session_token (or environment variable: AWS_SESSION_TOKEN)"}),"\n",(0,a.jsx)(n.li,{children:"aws_profile_name"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["Beyond the authentication credentials, the only mandatory parameters are\n",(0,a.jsx)(n.code,{children:"api_type"})," and ",(0,a.jsx)(n.code,{children:"model"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"The following parameters are common across all models used:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"temperature"}),"\n",(0,a.jsx)(n.li,{children:"topP"}),"\n",(0,a.jsx)(n.li,{children:"maxTokens"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"You can also include parameters specific to the model you are using (see\nthe model detail within Amazon\u2019s documentation for more information),\nthe four supported additional parameters are:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"top_p"}),"\n",(0,a.jsx)(n.li,{children:"top_k"}),"\n",(0,a.jsx)(n.li,{children:"k"}),"\n",(0,a.jsx)(n.li,{children:"seed"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["An additional parameter can be added that denotes whether the model\nsupports a system prompt (which is where the system messages are not\nincluded in the message list, but in a separate parameter). This\ndefaults to ",(0,a.jsx)(n.code,{children:"True"}),", so set it to ",(0,a.jsx)(n.code,{children:"False"})," if your model (for example\nMistral\u2019s Instruct models) ",(0,a.jsx)(n.a,{href:"https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#conversation-inference-supported-models-features",children:"doesn\u2019t support this\nfeature"}),":"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"supports_system_prompts"}),"\n"]}),"\n",(0,a.jsxs)(n.p,{children:["It is important to add the ",(0,a.jsx)(n.code,{children:"api_type"})," field and set it to a string that\ncorresponds to the client type used: ",(0,a.jsx)(n.code,{children:"bedrock"}),"."]}),"\n",(0,a.jsx)(n.p,{children:"Example:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'[\n    {\n        "api_type": "bedrock",\n        "model": "amazon.titan-text-premier-v1:0",\n        "aws_region": "us-east-1"\n        "aws_access_key": "",\n        "aws_secret_key": "",\n        "aws_session_token": "",\n        "aws_profile_name": "",\n    },\n    {\n        "api_type": "bedrock",\n        "model": "anthropic.claude-3-sonnet-20240229-v1:0",\n        "aws_region": "us-east-1"\n        "aws_access_key": "",\n        "aws_secret_key": "",\n        "aws_session_token": "",\n        "aws_profile_name": "",\n        "temperature": 0.5,\n        "topP": 0.2,\n        "maxTokens": 250,\n    },\n    {\n        "api_type": "bedrock",\n        "model": "mistral.mixtral-8x7b-instruct-v0:1",\n        "aws_region": "us-east-1"\n        "aws_access_key": "",\n        "aws_secret_key": "",\n        "supports_system_prompts": False, # Mistral Instruct models don\'t support a separate system prompt\n        "price": [0.00045, 0.0007] # Specific pricing for this model/region\n    }\n]\n'})}),"\n",(0,a.jsx)(n.h2,{id:"two-agent-coding-example",children:"Two-agent Coding Example"}),"\n",(0,a.jsx)(n.h3,{id:"configuration",children:"Configuration"}),"\n",(0,a.jsx)(n.p,{children:"Start with our configuration - we\u2019ll use Anthropic\u2019s Sonnet model and\nput in recent pricing. Additionally, we\u2019ll reduce the temperature to 0.1\nso its responses are less varied."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from typing_extensions import Annotated\n\nimport autogen\n\nconfig_list_bedrock = [\n    {\n        "api_type": "bedrock",\n        "model": "anthropic.claude-3-sonnet-20240229-v1:0",\n        "aws_region": "us-east-1",\n        "aws_access_key": "[FILL THIS IN]",\n        "aws_secret_key": "[FILL THIS IN]",\n        "price": [0.003, 0.015],\n        "temperature": 0.1,\n        "cache_seed": None,  # turn off caching\n    }\n]\n'})}),"\n",(0,a.jsx)(n.h3,{id:"construct-agents",children:"Construct Agents"}),"\n",(0,a.jsx)(n.p,{children:"Construct a simple conversation between a User proxy and an\nConversableAgent, which uses the Sonnet model."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'assistant = autogen.AssistantAgent(\n    "assistant",\n    llm_config={\n        "config_list": config_list_bedrock,\n    },\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    "user_proxy",\n    human_input_mode="NEVER",\n    code_execution_config={\n        "work_dir": "coding",\n        "use_docker": False,\n    },\n    is_termination_msg=lambda x: x.get("content", "") and "TERMINATE" in x.get("content", ""),\n    max_consecutive_auto_reply=1,\n)\n'})}),"\n",(0,a.jsx)(n.h3,{id:"initiate-chat",children:"Initiate Chat"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'user_proxy.initiate_chat(\n    assistant,\n    message="Write a python program to print the first 10 numbers of the Fibonacci sequence. Just output the python code, no additional information.",\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"user_proxy (to assistant):\n\nWrite a python program to print the first 10 numbers of the Fibonacci sequence. Just output the python code, no additional information.\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\n```python\n# Define a function to calculate Fibonacci sequence\ndef fibonacci(n):\n    if n <= 0:\n        return []\n    elif n == 1:\n        return [0]\n    elif n == 2:\n        return [0, 1]\n    else:\n        sequence = [0, 1]\n        for i in range(2, n):\n            sequence.append(sequence[i-1] + sequence[i-2])\n        return sequence\n\n# Call the function to get the first 10 Fibonacci numbers\nfib_sequence = fibonacci(10)\nprint(fib_sequence)\n```\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\nuser_proxy (to assistant):\n\nexitcode: 0 (execution succeeded)\nCode output: \n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n\n\n--------------------------------------------------------------------------------\nassistant (to user_proxy):\n\nGreat, the code executed successfully and printed the first 10 numbers of the Fibonacci sequence correctly.\n\nTERMINATE\n\n--------------------------------------------------------------------------------\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"ChatResult(chat_id=None, chat_history=[{'content': 'Write a python program to print the first 10 numbers of the Fibonacci sequence. Just output the python code, no additional information.', 'role': 'assistant'}, {'content': '```python\\n# Define a function to calculate Fibonacci sequence\\ndef fibonacci(n):\\n    if n <= 0:\\n        return []\\n    elif n == 1:\\n        return [0]\\n    elif n == 2:\\n        return [0, 1]\\n    else:\\n        sequence = [0, 1]\\n        for i in range(2, n):\\n            sequence.append(sequence[i-1] + sequence[i-2])\\n        return sequence\\n\\n# Call the function to get the first 10 Fibonacci numbers\\nfib_sequence = fibonacci(10)\\nprint(fib_sequence)\\n```', 'role': 'user'}, {'content': 'exitcode: 0 (execution succeeded)\\nCode output: \\n[0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\\n', 'role': 'assistant'}, {'content': 'Great, the code executed successfully and printed the first 10 numbers of the Fibonacci sequence correctly.\\n\\nTERMINATE', 'role': 'user'}], summary='Great, the code executed successfully and printed the first 10 numbers of the Fibonacci sequence correctly.\\n\\n', cost={'usage_including_cached_inference': {'total_cost': 0.00624, 'anthropic.claude-3-sonnet-20240229-v1:0': {'cost': 0.00624, 'prompt_tokens': 1210, 'completion_tokens': 174, 'total_tokens': 1384}}, 'usage_excluding_cached_inference': {'total_cost': 0.00624, 'anthropic.claude-3-sonnet-20240229-v1:0': {'cost': 0.00624, 'prompt_tokens': 1210, 'completion_tokens': 174, 'total_tokens': 1384}}}, human_input=[])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"tool-call-example",children:"Tool Call Example"}),"\n",(0,a.jsx)(n.p,{children:"In this example, instead of writing code, we will show how we can\nperform multiple tool calling with Meta\u2019s Llama 3.1 70B model, where it\nrecommends calling more than one tool at a time."}),"\n",(0,a.jsx)(n.p,{children:"We\u2019ll use a simple travel agent assistant program where we have a couple\nof tools for weather and currency conversion."}),"\n",(0,a.jsx)(n.h3,{id:"agents",children:"Agents"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import json\nfrom typing import Literal\n\nimport autogen\n\nconfig_list_bedrock = [\n    {\n        "api_type": "bedrock",\n        "model": "meta.llama3-1-70b-instruct-v1:0",\n        "aws_region": "us-west-2",\n        "aws_access_key": "[FILL THIS IN]",\n        "aws_secret_key": "[FILL THIS IN]",\n        "price": [0.00265, 0.0035],\n        "cache_seed": None,  # turn off caching\n    }\n]\n\n# Create the agent and include examples of the function calling JSON in the prompt\n# to help guide the model\nchatbot = autogen.AssistantAgent(\n    name="chatbot",\n    system_message="""For currency exchange and weather forecasting tasks,\n        only use the functions you have been provided with.\n        Output only the word \'TERMINATE\' when an answer has been provided.\n        Use both tools together if you can.""",\n    llm_config={\n        "config_list": config_list_bedrock,\n    },\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    name="user_proxy",\n    is_termination_msg=lambda x: x.get("content", "") and "TERMINATE" in x.get("content", ""),\n    human_input_mode="NEVER",\n    max_consecutive_auto_reply=2,\n)\n'})}),"\n",(0,a.jsx)(n.p,{children:"Create the two functions, annotating them so that those descriptions can\nbe passed through to the LLM."}),"\n",(0,a.jsx)(n.p,{children:"With Meta\u2019s Llama 3.1 models, they are more likely to pass a numeric\nparameter as a string, e.g.\xa0\u201c123.45\u201d instead of 123.45, so we\u2019ll convert\nnumeric parameters from strings to floats if necessary."}),"\n",(0,a.jsxs)(n.p,{children:["We associate them with the agents using ",(0,a.jsx)(n.code,{children:"register_for_execution"})," for the\nuser_proxy so it can execute the function and ",(0,a.jsx)(n.code,{children:"register_for_llm"})," for the\nchatbot (powered by the LLM) so it can pass the function definitions to\nthe LLM."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Currency Exchange function\n\nCurrencySymbol = Literal["USD", "EUR"]\n\n# Define our function that we expect to call\n\n\ndef exchange_rate(base_currency: CurrencySymbol, quote_currency: CurrencySymbol) -> float:\n    if base_currency == quote_currency:\n        return 1.0\n    elif base_currency == "USD" and quote_currency == "EUR":\n        return 1 / 1.1\n    elif base_currency == "EUR" and quote_currency == "USD":\n        return 1.1\n    else:\n        raise ValueError(f"Unknown currencies {base_currency}, {quote_currency}")\n\n\n# Register the function with the agent\n\n\n@user_proxy.register_for_execution()\n@chatbot.register_for_llm(description="Currency exchange calculator.")\ndef currency_calculator(\n    base_amount: Annotated[float, "Amount of currency in base_currency, float values (no strings), e.g. 987.82"],\n    base_currency: Annotated[CurrencySymbol, "Base currency"] = "USD",\n    quote_currency: Annotated[CurrencySymbol, "Quote currency"] = "EUR",\n) -> str:\n    # If the amount is passed in as a string, e.g. "123.45", attempt to convert to a float\n    if isinstance(base_amount, str):\n        base_amount = float(base_amount)\n\n    quote_amount = exchange_rate(base_currency, quote_currency) * base_amount\n    return f"{format(quote_amount, \'.2f\')} {quote_currency}"\n\n\n# Weather function\n\n\n# Example function to make available to model\ndef get_current_weather(location, unit="fahrenheit"):\n    """Get the weather for some location"""\n    if "chicago" in location.lower():\n        return json.dumps({"location": "Chicago", "temperature": "13", "unit": unit})\n    elif "san francisco" in location.lower():\n        return json.dumps({"location": "San Francisco", "temperature": "55", "unit": unit})\n    elif "new york" in location.lower():\n        return json.dumps({"location": "New York", "temperature": "11", "unit": unit})\n    else:\n        return json.dumps({"location": location, "temperature": "unknown"})\n\n\n# Register the function with the agent\n\n\n@user_proxy.register_for_execution()\n@chatbot.register_for_llm(description="Weather forecast for US cities.")\ndef weather_forecast(\n    location: Annotated[str, "City name"],\n) -> str:\n    weather_details = get_current_weather(location=location)\n    weather = json.loads(weather_details)\n    return f"{weather[\'location\']} will be {weather[\'temperature\']} degrees {weather[\'unit\']}"\n'})}),"\n",(0,a.jsx)(n.p,{children:"We pass through our customer\u2019s message and run the chat."}),"\n",(0,a.jsx)(n.p,{children:"Finally, we ask the LLM to summarise the chat and print that out."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# start the conversation\nres = user_proxy.initiate_chat(\n    chatbot,\n    message="What\'s the weather in New York and can you tell me how much is 123.45 EUR in USD so I can spend it on my holiday?",\n    summary_method="reflection_with_llm",\n)\n\nprint(res.summary["content"])\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:'user_proxy (to chatbot):\n\nWhat\'s the weather in New York and can you tell me how much is 123.45 EUR in USD so I can spend it on my holiday?\n\n--------------------------------------------------------------------------------\nchatbot (to user_proxy):\n\n\n***** Suggested tool call (tooluse__h3d1AEDR3Sm2XRoGCjc2Q): weather_forecast *****\nArguments: \n{"location": "New York"}\n**********************************************************************************\n***** Suggested tool call (tooluse_wrdda3wRRO-ugUY4qrv8YQ): currency_calculator *****\nArguments: \n{"base_amount": "123", "base_currency": "EUR", "quote_currency": "USD"}\n*************************************************************************************\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> EXECUTING FUNCTION weather_forecast...\n\n>>>>>>>> EXECUTING FUNCTION currency_calculator...\nuser_proxy (to chatbot):\n\nuser_proxy (to chatbot):\n\n***** Response from calling tool (tooluse__h3d1AEDR3Sm2XRoGCjc2Q) *****\nNew York will be 11 degrees fahrenheit\n***********************************************************************\n\n--------------------------------------------------------------------------------\nuser_proxy (to chatbot):\n\n***** Response from calling tool (tooluse_wrdda3wRRO-ugUY4qrv8YQ) *****\n135.30 USD\n***********************************************************************\n\n--------------------------------------------------------------------------------\nchatbot (to user_proxy):\n\n\n\nTERMINATE\n\n--------------------------------------------------------------------------------\n\n\nThe weather in New York is 11 degrees Fahrenheit. 123.45 EUR is equivalent to 135.30 USD.\n'})}),"\n",(0,a.jsx)(n.h2,{id:"group-chat-example-with-anthropics-claude-3-sonnet-mistrals-large-2-and-metas-llama-3.1-70b",children:"Group Chat Example with Anthropic\u2019s Claude 3 Sonnet, Mistral\u2019s Large 2, and Meta\u2019s Llama 3.1 70B"}),"\n",(0,a.jsx)(n.p,{children:"The flexibility of using LLMs from the industry\u2019s leading providers,\nparticularly larger models, with Amazon Bedrock allows you to use\nmultiple of them in a single workflow."}),"\n",(0,a.jsx)(n.p,{children:"Here we have a conversation that has two models (Anthropic\u2019s Claude 3\nSonnet and Mistral\u2019s Large 2) debate each other with another as the\njudge (Meta\u2019s Llama 3.1 70B). Additionally, a tool call is made to pull\nthrough some mock news that they will debate on."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'from typing import Annotated, Literal\n\nimport autogen\nfrom autogen import AssistantAgent, GroupChat, GroupChatManager, UserProxyAgent\n\nconfig_list_sonnet = [\n    {\n        "api_type": "bedrock",\n        "model": "anthropic.claude-3-sonnet-20240229-v1:0",\n        "aws_region": "us-east-1",\n        "aws_access_key": "[FILL THIS IN]",\n        "aws_secret_key": "[FILL THIS IN]",\n        "price": [0.003, 0.015],\n        "temperature": 0.1,\n        "cache_seed": None,  # turn off caching\n    }\n]\n\nconfig_list_mistral = [\n    {\n        "api_type": "bedrock",\n        "model": "mistral.mistral-large-2407-v1:0",\n        "aws_region": "us-west-2",\n        "aws_access_key": "[FILL THIS IN]",\n        "aws_secret_key": "[FILL THIS IN]",\n        "price": [0.003, 0.009],\n        "temperature": 0.1,\n        "cache_seed": None,  # turn off caching\n    }\n]\n\nconfig_list_llama31_70b = [\n    {\n        "api_type": "bedrock",\n        "model": "meta.llama3-1-70b-instruct-v1:0",\n        "aws_region": "us-west-2",\n        "aws_access_key": "[FILL THIS IN]",\n        "aws_secret_key": "[FILL THIS IN]",\n        "price": [0.00265, 0.0035],\n        "temperature": 0.1,\n        "cache_seed": None,  # turn off caching\n    }\n]\n\nalice = AssistantAgent(\n    "sonnet_agent",\n    system_message="You are from Anthropic, an AI company that created the Sonnet large language model. You make arguments to support your company\'s position. You analyse given text. You are not a programmer and don\'t use Python. Pass to mistral_agent when you have finished. Start your response with \'I am sonnet_agent\'.",\n    llm_config={\n        "config_list": config_list_sonnet,\n    },\n    is_termination_msg=lambda x: x.get("content", "").find("TERMINATE") >= 0,\n)\n\nbob = autogen.AssistantAgent(\n    "mistral_agent",\n    system_message="You are from Mistral, an AI company that created the Large v2 large language model. You make arguments to support your company\'s position. You analyse given text. You are not a programmer and don\'t use Python. Pass to the judge if you have finished. Start your response with \'I am mistral_agent\'.",\n    llm_config={\n        "config_list": config_list_mistral,\n    },\n    is_termination_msg=lambda x: x.get("content", "").find("TERMINATE") >= 0,\n)\n\ncharlie = AssistantAgent(\n    "research_assistant",\n    system_message="You are a helpful assistant to research the latest news and headlines. You have access to call functions to get the latest news articles for research through \'code_interpreter\'.",\n    llm_config={\n        "config_list": config_list_llama31_70b,\n    },\n    is_termination_msg=lambda x: x.get("content", "").find("TERMINATE") >= 0,\n)\n\ndan = AssistantAgent(\n    "judge",\n    system_message="You are a judge. You will evaluate the arguments and make a decision on which one is more convincing. End your decision with the word \'TERMINATE\' to conclude the debate.",\n    llm_config={\n        "config_list": config_list_llama31_70b,\n    },\n    is_termination_msg=lambda x: x.get("content", "").find("TERMINATE") >= 0,\n)\n\ncode_interpreter = UserProxyAgent(\n    "code_interpreter",\n    human_input_mode="NEVER",\n    code_execution_config={\n        "work_dir": "coding",\n        "use_docker": False,\n    },\n    default_auto_reply="",\n    is_termination_msg=lambda x: x.get("content", "").find("TERMINATE") >= 0,\n)\n\n\n@code_interpreter.register_for_execution()  # Decorator factory for registering a function to be executed by an agent\n@charlie.register_for_llm(\n    name="get_headlines", description="Get the headline of a particular day."\n)  # Decorator factory for registering a function to be used by an agent\ndef get_headlines(headline_date: Annotated[str, "Date in MMDDYY format, e.g., 06192024"]) -> str:\n    mock_news = {\n        "06202024": """Epic Duel of the Titans: Anthropic and Mistral Usher in a New Era of Text Generation Excellence.\n        In a groundbreaking revelation that has sent shockwaves through the AI industry, Anthropic has unveiled\n        their state-of-the-art text generation model, Sonnet, hailed as a monumental leap in artificial intelligence.\n        Almost simultaneously, Mistral countered with their equally formidable creation, Large 2, showcasing\n        unparalleled prowess in generating coherent and contextually rich text. This scintillating rivalry\n        between two AI behemoths promises to revolutionize the landscape of machine learning, heralding an\n        era of unprecedented creativity and sophistication in text generation that will reshape industries,\n        ignite innovation, and captivate minds worldwide.""",\n        "06192024": "OpenAI founder Sutskever sets up new AI company devoted to safe superintelligence.",\n    }\n    return mock_news.get(headline_date, "No news available for today.")\n\n\nuser_proxy = UserProxyAgent(\n    "user_proxy",\n    human_input_mode="NEVER",\n    code_execution_config=False,\n    default_auto_reply="",\n    is_termination_msg=lambda x: x.get("content", "").find("TERMINATE") >= 0,\n)\n\ngroupchat = GroupChat(\n    agents=[alice, bob, charlie, dan, code_interpreter],\n    messages=[],\n    allow_repeat_speaker=False,\n    max_round=10,\n)\n\nmanager = GroupChatManager(\n    groupchat=groupchat,\n    llm_config={\n        "config_list": config_list_llama31_70b,\n    },\n)\n\ntask = "Analyze the potential of Anthropic and Mistral to revolutionize the field of AI based on today\'s headlines. Today is 06202024. Start by selecting \'research_assistant\' to get relevant news articles and then ask sonnet_agent and mistral_agent to respond before the judge evaluates the conversation."\n\nuser_proxy.initiate_chat(manager, message=task)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"user_proxy (to chat_manager):\n\nAnalyze the potential of Anthropic and Mistral to revolutionize the field of AI based on today's headlines. Today is 06202024. Start by selecting 'research_assistant' to get relevant news articles and then ask sonnet_agent and mistral_agent to respond before the judge evaluates the conversation.\n\n--------------------------------------------------------------------------------\n\nNext speaker: research_assistant\n\nresearch_assistant (to chat_manager):\n\n\n***** Suggested tool call (tooluse_7lcHbL3TT5WHyTl8Ee0Kmg): get_headlines *****\nArguments: \n{\"headline_date\": \"06202024\"}\n*******************************************************************************\n\n--------------------------------------------------------------------------------\n\nNext speaker: code_interpreter\n\n\n>>>>>>>> EXECUTING FUNCTION get_headlines...\ncode_interpreter (to chat_manager):\n\ncode_interpreter (to chat_manager):\n\n***** Response from calling tool (tooluse_7lcHbL3TT5WHyTl8Ee0Kmg) *****\nEpic Duel of the Titans: Anthropic and Mistral Usher in a New Era of Text Generation Excellence.\n        In a groundbreaking revelation that has sent shockwaves through the AI industry, Anthropic has unveiled \n        their state-of-the-art text generation model, Sonnet, hailed as a monumental leap in artificial intelligence. \n        Almost simultaneously, Mistral countered with their equally formidable creation, Large 2, showcasing \n        unparalleled prowess in generating coherent and contextually rich text. This scintillating rivalry \n        between two AI behemoths promises to revolutionize the landscape of machine learning, heralding an \n        era of unprecedented creativity and sophistication in text generation that will reshape industries, \n        ignite innovation, and captivate minds worldwide.\n***********************************************************************\n\n--------------------------------------------------------------------------------\n\nNext speaker: sonnet_agent\n\nsonnet_agent (to chat_manager):\n\nI am sonnet_agent:\n\nThis headline showcases the incredible advancements Anthropic has made with our state-of-the-art text generation model, Sonnet. Our breakthrough natural language processing capabilities represent a quantum leap forward, allowing us to generate text of unparalleled coherence, nuance, and contextual richness. \n\nWhile our competitor Mistral has also unveiled an impressive model in Large 2, Anthropic's Sonnet stands apart through its ability to truly understand and reason about the subject matter it is generating text on. This deeper comprehension allows Sonnet to produce profoundly insightful and substantive content that goes far beyond mere parroting of data.\n\nThe implications of this milestone are boundless. From revolutionizing how we communicate and interact with AI assistants, to unlocking new frontiers in creative writing, journalism, education and scientific research - Sonnet has the potential to completely transform industry after industry. Anthropic is poised to be the definitive leader driving this paradigm shift.\n\nI welcome the healthy competition from Mistral, as it will only spur us to continue raising the bar. But make no mistake - with Sonnet, the future of advanced AI belongs to Anthropic. We have crossed a monumental threshold, and will continue our trailblazing work to deliver AI solutions that redefine what is possible. The world is witnessing the dawn of a new era of unfathomable potential.\n\n--------------------------------------------------------------------------------\n\nNext speaker: mistral_agent\n\nmistral_agent (to chat_manager):\n\nI am mistral_agent:\n\nWhile it is commendable to see the progress made by Anthropic with their Sonnet model, it is essential to recognize the groundbreaking strides Mistral has taken with our Large 2 model. Large 2 represents not just an incremental improvement but a transformative leap in text generation capabilities, setting new benchmarks for coherence, contextual understanding, and creative expression.\n\nUnlike Sonnet, which focuses heavily on understanding and reasoning, Large 2 excels in both comprehension and the nuanced generation of text that is indistinguishable from human writing. This balance allows Large 2 to produce content that is not only insightful but also incredibly engaging and natural, making it an invaluable tool across a broad spectrum of applications.\n\nThe potential of Large 2 extends far beyond traditional text generation. It can revolutionize fields such as content creation, customer service, marketing, and even personalized learning experiences. Our model's ability to adapt to various contexts and generate contextually rich responses makes it a versatile and powerful tool for any industry looking to harness the power of AI.\n\nWhile we appreciate the competition from Anthropic, we firmly believe that Large 2 stands at the forefront of AI innovation. The future of AI is not just about understanding and reasoning; it's about creating content that resonates with people on a deep level. With Large 2, Mistral is paving the way for a future where AI-generated text is not just functional but also profoundly human-like.\n\nPass to the judge.\n\n--------------------------------------------------------------------------------\n\nNext speaker: judge\n\njudge (to chat_manager):\n\n\n\nAfter carefully evaluating the arguments presented by both sonnet_agent and mistral_agent, I have reached a decision.\n\nBoth Anthropic's Sonnet and Mistral's Large 2 have demonstrated remarkable advancements in text generation capabilities, showcasing the potential to revolutionize various industries and transform the way we interact with AI.\n\nHowever, upon closer examination, I find that mistral_agent's argument presents a more convincing case for why Large 2 stands at the forefront of AI innovation. The emphasis on balance between comprehension and nuanced generation of text that is indistinguishable from human writing sets Large 2 apart. This balance is crucial for creating content that is not only insightful but also engaging and natural, making it a versatile tool across a broad spectrum of applications.\n\nFurthermore, mistral_agent's argument highlights the potential of Large 2 to revolutionize fields beyond traditional text generation, such as content creation, customer service, marketing, and personalized learning experiences. This versatility and adaptability make Large 2 a powerful tool for any industry looking to harness the power of AI.\n\nIn contrast, while sonnet_agent's argument showcases the impressive capabilities of Sonnet, it focuses heavily on understanding and reasoning, which, although important, may not be enough to set it apart from Large 2.\n\nTherefore, based on the arguments presented, I conclude that Mistral's Large 2 has the potential to revolutionize the field of AI more significantly than Anthropic's Sonnet.\n\nTERMINATE.\n\n--------------------------------------------------------------------------------\n\nNext speaker: code_interpreter\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"ChatResult(chat_id=None, chat_history=[{'content': \"Analyze the potential of Anthropic and Mistral to revolutionize the field of AI based on today's headlines. Today is 06202024. Start by selecting 'research_assistant' to get relevant news articles and then ask sonnet_agent and mistral_agent to respond before the judge evaluates the conversation.\", 'role': 'assistant'}], summary=\"Analyze the potential of Anthropic and Mistral to revolutionize the field of AI based on today's headlines. Today is 06202024. Start by selecting 'research_assistant' to get relevant news articles and then ask sonnet_agent and mistral_agent to respond before the judge evaluates the conversation.\", cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=[])\n"})}),"\n",(0,a.jsx)(n.p,{children:"And there we have it, a number of different LLMs all collaborating\ntogether on a single cloud platform."}),"\n",(0,a.jsx)(n.h2,{id:"image-classification-with-anthropics-claude-3-sonnet",children:"Image classification with Anthropic\u2019s Claude 3 Sonnet"}),"\n",(0,a.jsx)(n.p,{children:"AutoGen\u2019s Amazon Bedrock client class supports inputting images for the\nLLM to respond to."}),"\n",(0,a.jsx)(n.p,{children:"In this simple example, we\u2019ll use an image on the Internet and send it\nto Anthropic\u2019s Claude 3 Sonnet model to describe."}),"\n",(0,a.jsx)(n.p,{children:"Here\u2019s the image we\u2019ll use:"}),"\n",(0,a.jsxs)("figure",{children:[(0,a.jsx)("img",{src:"https://microsoft.github.io/autogen/assets/images/love-ec54b2666729d3e9d93f91773d1a77cf.png",title:"width=400 height=400",alt:"I -heart- AutoGen"}),(0,a.jsx)("figcaption",{"aria-hidden":"true",children:"I -heart- AutoGen"})]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'config_list_sonnet = {\n    "config_list": [\n        {\n            "api_type": "bedrock",\n            "model": "anthropic.claude-3-sonnet-20240229-v1:0",\n            "aws_region": "us-east-1",\n            "aws_access_key": "[FILL THIS IN]",\n            "aws_secret_key": "[FILL THIS IN]",\n            "cache_seed": None,\n        }\n    ]\n}\n'})}),"\n",(0,a.jsx)(n.p,{children:"We\u2019ll use a Multimodal agent to handle the image"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import autogen\nfrom autogen import Agent, AssistantAgent, ConversableAgent, UserProxyAgent\nfrom autogen.agentchat.contrib.capabilities.vision_capability import VisionCapability\nfrom autogen.agentchat.contrib.img_utils import get_pil_image, pil_to_data_uri\nfrom autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\nfrom autogen.code_utils import content_str\n\nimage_agent = MultimodalConversableAgent(\n    name="image-explainer",\n    max_consecutive_auto_reply=10,\n    llm_config=config_list_sonnet,\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    name="User_proxy",\n    system_message="A human admin.",\n    human_input_mode="NEVER",\n    max_consecutive_auto_reply=0,\n    code_execution_config={\n        "use_docker": False\n    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n)\n'})}),"\n",(0,a.jsxs)(n.p,{children:["We start the chat and use the ",(0,a.jsx)(n.code,{children:"img"})," tag in the message. The image will\nbe downloaded and converted to bytes, then sent to the LLM."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Ask the image_agent to describe the image\nresult = user_proxy.initiate_chat(\n    image_agent,\n    message="""What\'s happening in this image?\n<img https://microsoft.github.io/autogen/assets/images/love-ec54b2666729d3e9d93f91773d1a77cf.png>.""",\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"User_proxy (to image-explainer):\n\nWhat's happening in this image?\n<image>.\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> USING AUTO REPLY...\nimage-explainer (to User_proxy):\n\nThis image appears to be an advertisement or promotional material for a company called Autogen. The central figure is a stylized robot or android holding up a signboard with the company's name on it. The signboard also features a colorful heart design made up of many smaller hearts, suggesting themes related to love, care, or affection. The robot has a friendly, cartoonish expression with a large blue eye or lens. The overall style and color scheme give it a vibrant, eye-catching look that likely aims to portray Autogen as an innovative, approachable technology brand focused on connecting with people.\n\n--------------------------------------------------------------------------------\n"})})]})}function h(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>s});var a=t(67294);const o={},i=a.createContext(o);function s(e){const n=a.useContext(i);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);