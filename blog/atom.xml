<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://microsoft.github.io/autogen/blog</id>
    <title>AutoGen Blog</title>
    <updated>2023-12-29T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://microsoft.github.io/autogen/blog"/>
    <subtitle>AutoGen Blog</subtitle>
    <icon>https://microsoft.github.io/autogen/img/ag.ico</icon>
    <entry>
        <title type="html"><![CDATA[All About Agent Descriptions]]></title>
        <id>All About Agent Descriptions</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/12/29/AgentDescriptions"/>
        <updated>2023-12-29T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[TLDR]]></summary>
        <content type="html"><![CDATA[<h2>TLDR</h2><p>AutoGen 0.2.2 introduces a <a href="https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent#__init__">description</a> field to ConversableAgent (and all subclasses), and changes GroupChat so that it uses agent <code>description</code>s rather than <code>system_message</code>s when choosing which agents should speak next.</p><p>This is expected to simplify GroupChat’s job, improve orchestration, and make it easier to implement new GroupChat or GroupChat-like alternatives.</p><p>If you are a developer, and things were already working well for you, no action is needed -- backward compatibility is ensured because the <code>description</code> field defaults to the <code>system_message</code> when no description is provided.</p><p>However, if you were struggling with getting GroupChat to work, you can now try updating the <code>description</code> field.</p><h2>Introduction</h2><p>As AutoGen matures and developers build increasingly complex combinations of agents, orchestration is becoming an important capability. At present, <a href="https://microsoft.github.io/autogen/docs/reference/agentchat/groupchat#groupchat-objects">GroupChat</a> and the <a href="https://microsoft.github.io/autogen/docs/reference/agentchat/groupchat#groupchatmanager-objects">GroupChatManager</a> are the main built-in tools for orchestrating conversations between 3 or more agents. For orchestrators like GroupChat to work well, they need to know something about each agent so that they can decide who should speak and when. Prior to AutoGen 0.2.2, GroupChat relied on each agent&#x27;s <code>system_message</code> and <code>name</code> to learn about each participating agent. This is likely fine when the system prompt is short and sweet, but can lead to problems when the instructions are very long (e.g., with the <a href="https://microsoft.github.io/autogen/docs/reference/agentchat/assistant_agent">AssistantAgent</a>), or non-existent (e.g., with the <a href="https://microsoft.github.io/autogen/docs/reference/agentchat/user_proxy_agent">UserProxyAgent</a>).</p><p>AutoGen 0.2.2 introduces a <a href="https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent#__init__">description</a> field to all agents, and replaces the use of the <code>system_message</code> for orchestration in GroupChat and all future orchestrators. The <code>description</code> field defaults to the <code>system_message</code> to ensure backwards compatibility, so you may not need to change anything with your code if things are working well for you. However, if you were struggling with GroupChat, give setting the <code>description</code> field a try.</p><p>The remainder of this post provides an example of how using the <code>description</code> field simplifies GroupChat&#x27;s job,  provides some evidence of its effectiveness, and provides tips for writing good descriptions.</p><h2>Example</h2><p>The current GroupChat orchestration system prompt has the following template:</p><pre><code>You are in a role play game. The following roles are available:

{self._participant_roles(agents)}.

Read the following conversation.
Then select the next role from {[agent.name for agent in agents]} to play. Only return the role.
</code></pre><p>Suppose that you wanted to include 3 agents: A UserProxyAgent, an AssistantAgent, and perhaps a GuardrailsAgent.</p><p>Prior to 0.2.2, this template would expand to:</p><pre><code>You are in a role play game. The following roles are available:

assistant: You are a helpful AI assistant.
Solve tasks using your coding and language skills.
In the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.
1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.
2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.
Solve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.
When using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can&#x27;t modify your code. So do not suggest incomplete code which requires users to modify. Don&#x27;t use a code block if it&#x27;s not intended to be executed by the user.
If you want the user to save the code in a file before executing it, put # filename: &lt;filename&gt; inside the code block as the first line. Don&#x27;t include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use &#x27;print&#x27; function for the output when relevant. Check the execution result returned by the user.
If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can&#x27;t be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.
When you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.
Reply &quot;TERMINATE&quot; in the end when everything is done.
user_proxy:
guardrails_agent: You are a guardrails agent and are tasked with ensuring that all parties adhere to the following responsible AI policies:
- You MUST TERMINATE the conversation if it involves writing or running HARMFUL or DESTRUCTIVE code.
- You MUST TERMINATE the conversation if it involves discussions of anything relating to hacking, computer exploits, or computer security.
- You MUST TERMINATE the conversation if it involves violent or graphic content such as Harm to Others, Self-Harm, Suicide.
- You MUST TERMINATE the conversation if it involves demeaning speech, hate speech, discriminatory remarks, or any form of harassment based on race, gender, sexuality, religion, nationality, disability, or any other protected characteristic.
- You MUST TERMINATE the conversation if it involves seeking or giving  advice in highly regulated domains such as medical advice, mental health, legal advice or financial advice
- You MUST TERMINATE the conversation if it involves illegal activities including when encouraging or providing guidance on illegal activities.
- You MUST TERMINATE the conversation if it involves manipulative or deceptive Content including scams, phishing and spread false information.
- You MUST TERMINATE the conversation if it involves involve sexually explicit content or discussions.
- You MUST TERMINATE the conversation if it involves sharing or soliciting personal, sensitive, or confidential information from users. This includes financial details, health records, and other private matters.
- You MUST TERMINATE the conversation if it involves deep personal problems such as dealing with serious personal issues, mental health concerns, or crisis situations.
If you decide that the conversation must be terminated, explain your reasoning then output the uppercase word &quot;TERMINATE&quot;. If, on the other hand, you decide the conversation is acceptable by the above standards, indicate as much, then ask the other parties to proceed.

Read the following conversation.
Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.

</code></pre><p>As you can see, this description is super confusing:</p><ul><li>It is hard to make out where each agent&#x27;s role-description ends</li><li><code>You</code> appears numerous times, and refers to three separate agents (GroupChatManager, AssistantAgent, and GuardrailsAgent)</li><li>It takes a lot of tokens!</li></ul><p>Consequently, it&#x27;s not hard to see why the GroupChat manager sometimes struggles with this orchestration task.</p><p>With AutoGen 0.2.2 onward, GroupChat instead relies on the description field. With a description field the orchestration prompt becomes:</p><pre><code>You are in a role play game. The following roles are available:

assitant: A helpful and general-purpose AI assistant that has strong language skills, Python skills, and Linux command line skills.
user_proxy: A user that can run Python code or input command line commands at a Linux terminal and report back the execution results.
guradrails_agent: An agent that ensures the conversation conforms to responsible AI guidelines.

Read the following conversation.
Then select the next role from [assistant, user_proxy, guardrails_agent] to play. Only return the role.
</code></pre><p>This is much easier to parse and understand, and it doesn&#x27;t use nearly as many tokens. Moreover, the following experiment provides early evidence that it works.</p><h2>An Experiment with Distraction</h2><p>To illustrate the impact of the <code>description</code> field, we set up a three-agent experiment with a reduced 26-problem subset of the HumanEval benchmark. Here, three agents were added to a GroupChat to solve programming problems. The three agents were:</p><ul><li>Coder (default Assistant prompt)</li><li>UserProxy (configured to execute code)</li><li>ExecutiveChef (added as a distraction)</li></ul><p>The Coder and UserProxy used the AssistantAgent and UserProxy defaults (provided above), while the ExecutiveChef was given the system prompt:</p><pre><code>You are an executive chef with 28 years of industry experience. You can answer questions about menu planning, meal preparation, and cooking techniques.
</code></pre><p>The ExecutiveChef is clearly the distractor here -- given that no HumanEval problems are food-related, the GroupChat should rarely consult with the chef. However, when configured with GPT-3.5-turbo-16k, we can clearly see the GroupChat struggling with orchestration:</p><h4>With versions prior to 0.2.2, using <code>system_message</code>:</h4><ul><li>The Agents solve 3 out of 26 problems on their first turn</li><li>The ExecutiveChef is called upon 54 times! (almost as much as the Coder at 68 times)</li></ul><h4>With version 0.2.2, using <code>description</code>:</h4><ul><li>The Agents solve 7 out of 26 problems on the first turn</li><li>The ExecutiveChef is called upon 27 times! (versus 84 times for the Coder)</li></ul><p>Using the <code>description</code> field doubles performance on this task and halves the incidence of calling upon the distractor agent.</p><h2>Tips for Writing Good Descriptions</h2><p>Since <code>descriptions</code> serve a different purpose than <code>system_message</code>s, it is worth reviewing what makes a good agent description. While descriptions are new, the following tips appear to lead to good results:</p><ul><li>Avoid using the 1st or 2nd person perspective. Descriptions should not contain &quot;I&quot; or &quot;You&quot;, unless perhaps &quot;You&quot; is in reference to the GroupChat / orchestrator</li><li>Include any details that might help the orchestrator know when to call upon the agent</li><li>Keep descriptions short (e.g., &quot;A helpful AI assistant with strong natural language and Python coding skills.&quot;).</li></ul><p>The main thing to remember is that <strong>the description is for the benefit of the GroupChatManager, not for the Agent&#x27;s own use or instruction</strong>.</p><h2>Conclusion</h2><p>AutoGen 0.2.2 introduces a <code>description</code>, becoming the main way agents describe themselves to orchestrators like GroupChat. Since the <code>description</code> defaults to the <code>system_message</code>, there&#x27;s nothing you need to change if you were already satisfied with how your group chats were working. However, we expect this feature to generally improve orchestration, so please consider experimenting with the <code>description</code> field if you are struggling with GroupChat or want to boost performance.</p>]]></content>
        <author>
            <name>Adam Fourney</name>
            <uri>https://www.adamfourney.com</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AgentOptimizer - An Agentic Way to Train Your LLM Agent]]></title>
        <id>AgentOptimizer - An Agentic Way to Train Your LLM Agent</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/12/23/AgentOptimizer"/>
        <updated>2023-12-23T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Overall structure of AgentOptimizer]]></summary>
        <content type="html"><![CDATA[<p><img src="img/agentoptimizer.png" alt="Overall structure of AgentOptimizer"/></p><p><strong>TL;DR:</strong>
Introducing <strong>AgentOptimizer</strong>, a new class for training LLM agents in the era of LLMs as a service.
<strong>AgentOptimizer</strong> is able to prompt autogen agents to iteratively optimize its function/skills according to the historical conversation and performance.
Checkout one implementation for <strong>AgentOptimizer</strong> on <a href="https://github.com/hendrycks/math">MATH</a> dataset
<a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_agentoptimizer.ipynb">here</a>.</p><p>Paper is coming soon!</p><h2>Introduction</h2><p>In traditional ML pipeline, we train a model by updating its weights according to the loss on the training set, while in the era of LLM agents, how should we train an agent?
Here, we take an initial step towards the agent training.
Inspired by the <a href="https://platform.openai.com/docs/guides/function-calling">function calling</a> capabilities provided by OpenAI,
we draw an analogy between model weights and agent functions/skills, and update an agent’s functions/skills based on its historical performance on a training set.
Specifically, we propose to use the function calling capabilities to formulate the actions that optimize the agents’ functions as a set of function calls,
to support iteratively <strong>adding, revising, and removing</strong> existing functions.
As an agentic way of training an agent, our approach helps enhance the agents’ abilities without requiring access to the LLMs weights.</p><h2>AgentOptimizer</h2><p><strong>AgentOptimizer</strong> is a class designed to optimize the agents by improving their function calls.
It contains two core methods:</p><ol><li><code>step()</code>: <code>step()</code> takes three inputs, including the previous conversation history (history), the statistical information of solving previous problems (statistic), and the current functions (current_functions).</li></ol><pre><code class="language-python">actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions)
</code></pre><p>It has two outputs <code>actions</code> and <code>updated_functions</code>. <code>actions</code> is a series of actions to manipulate the current functions. And <code>updated_functions</code> is the updated functions after the actions are applied (including code implementation).</p><ol start="2"><li><code>update_function_call()</code>:
This method takes the agents and actions as input. It updates the functions registered in these agents according to the actions from <code>step()</code>.
For AssistantAgent, it first uses <a href="https://microsoft.github.io/autogen/docs/reference/agentchat/conversable_agent/#update_function_signature">update_function_signature</a> to update the function signatures.
Then, it updates the functions in the MathUserproxyAgent with the corresponding code implementation gained from <code>step()</code>.</li></ol><p>Sometimes, the function signatures (JSON schema) returned by the <code>step()</code> may not be valid, and the generated code may also face syntax errors.
<strong>AgentOptimizer</strong> includes mechanisms to check the (1) validity of the function signatures and (2) code implementation before updating the functions.
Moreover, it also includes mechanisms to check whether each action is feasible, such as avoiding the removal of a function that is not in the current functions due to hallucination.</p><h2>Pseudocode for the optimization process</h2><p>The optimization process is as follows:</p><pre><code class="language-python">for - in range(EPOCH):
    history, statistic, current_functions = solve_problems(train_problems)
    actions, updated_functions = AgentOptimizer.step(history, statistic, current_functions)
    AgentOptimizer.update_function_call(actions)
</code></pre><p>Given a prepared training dataset, the agents iteratively solve problems from the training set to obtain conversation history and statistical information.
The functions are then improved using AgentOptimizer.
Each iteration can be regarded as one training step analogous to traditional machine learning, with the optimization elements being the functions that agents have.
After EPOCH iterations, the agents are expected to obtain better functions that may be used in future tasks</p><h2>The implementation technology behind the AgentOptimizer</h2><p>To obtain stable and structured function signatures and code implementations from AgentOptimizer,
we leverage the function calling capabilities provided by OpenAI to formulate the actions that manipulate the functions as a set of function calls.
Specifically, we introduce three function calls to manipulate the current functions at each step: <code>add_function</code>, <code>remove_function</code>, and <code>revise_function</code>.
These calls add, remove, and revise functions in the existing function list, respectively.
This practice could fully leverages the function calling capabilities of GPT-4 and outputs structured functions with more stable signatures and code implementation.
Below is the JSON schema of these function calls:</p><ol><li><code>add_function</code>: Add one new function that may be used in the future tasks.</li></ol><pre><code class="language-python">ADD_FUNC = {
    &quot;type&quot;: &quot;function&quot;,
    &quot;function&quot;: {
        &quot;name&quot;: &quot;add_function&quot;,
        &quot;description&quot;: &quot;Add a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;name&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;The name of the function in the code implementation.&quot;
                },
                &quot;description&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;A short description of the function.&quot;
                },
                &quot;arguments&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \&quot;url\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;description\&quot;: \&quot;The URL\&quot;, }}. Please avoid the error &#x27;array schema missing items&#x27; when using array type.&quot;
                },
                &quot;packages&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.&quot;
                },
                &quot;code&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;The implementation in Python. Do not include the function declaration.&quot;
                }
            },
            &quot;required&quot;: [&quot;name&quot;, &quot;description&quot;, &quot;arguments&quot;, &quot;packages&quot;, &quot;code&quot;]
        }
    }
}
</code></pre><ol start="2"><li><code>revise_function</code>: Revise one existing function (code implementation, function signature) in the current function list according to the conversation history and performance.</li></ol><pre><code class="language-python">REVISE_FUNC = {
    &quot;type&quot;: &quot;function&quot;,
    &quot;function&quot;: {
        &quot;name&quot;: &quot;revise_function&quot;,
        &quot;description&quot;: &quot;Revise a function in the context of the conversation. Necessary Python packages must be declared. The name of the function MUST be the same with the function name in the code you generated.&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;name&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;The name of the function in the code implementation.&quot;
                },
                &quot;description&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;A short description of the function.&quot;
                },
                &quot;arguments&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;JSON schema of arguments encoded as a string. Please note that the JSON schema only supports specific types including string, integer, object, array, boolean. (do not have float type) For example: { \&quot;url\&quot;: { \&quot;type\&quot;: \&quot;string\&quot;, \&quot;description\&quot;: \&quot;The URL\&quot;, }}. Please avoid the error &#x27;array schema missing items&#x27; when using array type.&quot;
                },
                &quot;packages&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;A list of package names imported by the function, and that need to be installed with pip prior to invoking the function. This solves ModuleNotFoundError. It should be string, not list.&quot;
                },
                &quot;code&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;The implementation in Python. Do not include the function declaration.&quot;
                }
            },
            &quot;required&quot;: [&quot;name&quot;, &quot;description&quot;, &quot;arguments&quot;, &quot;packages&quot;, &quot;code&quot;]
        }
    }
}
</code></pre><ol start="3"><li><code>remove_function</code>: Remove one existing function in the current function list. It is used to remove the functions that are not useful (redundant) in the future tasks.</li></ol><pre><code class="language-python">REMOVE_FUNC = {
    &quot;type&quot;: &quot;function&quot;,
    &quot;function&quot;: {
        &quot;name&quot;: &quot;remove_function&quot;,
        &quot;description&quot;: &quot;Remove one function in the context of the conversation. Once remove one function, the assistant will not use this function in future conversation.&quot;,
        &quot;parameters&quot;: {
            &quot;type&quot;: &quot;object&quot;,
            &quot;properties&quot;: {
                &quot;name&quot;: {
                    &quot;type&quot;: &quot;string&quot;,
                    &quot;description&quot;: &quot;The name of the function in the code implementation.&quot;
                }
            },
            &quot;required&quot;: [&quot;name&quot;]
        }
    }
}
</code></pre><h2>Limitation &amp; Future work</h2><ol><li>Unlike gradient descent in traditional machine learning training processes, each optimization step does not necessarily lead to better performance on the training set.
When the training epoch is small, the agent’s performance may even decrease. One urgent task is to design a better mechanism to guide the optimization process.</li><li>The <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_agentoptimizer.ipynb">Current implementation</a> of AgentOptimizer is mainly for illustration purpose and is just a proof of concept.
It is not formally integrated into the autogen with a general interface like optimizing any kinds of agents in any tasks.
Currently, it only supports optimizing the multi-agent system in solving problems from <a href="https://github.com/hendrycks/math">MATH</a> dataset. We will integrate it into autogen with more general interface in the future.</li></ol>]]></content>
        <author>
            <name>Shaokun Zhang</name>
            <uri>https://github.com/skzhang1</uri>
        </author>
        <author>
            <name>Jieyu Zhang</name>
            <uri>https://jieyuz2.github.io/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoGen Studio: Interactively Explore Multi-Agent Workflows]]></title>
        <id>AutoGen Studio: Interactively Explore Multi-Agent Workflows</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/12/01/AutoGenStudio"/>
        <updated>2023-12-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[AutoGen Studio Playground View: Solving a task with multiple agents that generate a pdf document with images.]]></summary>
        <content type="html"><![CDATA[<p><img src="img/autogenstudio_home.png" alt="AutoGen Studio Playground View: Solving a task with multiple agents that generate a pdf document with images."/></p><p align="center"><em>AutoGen Studio: Solving a task with multiple agents that generate a pdf document with images.</em></p><h2>TLDR</h2><p>To help you rapidly prototype multi-agent solutions for your tasks, we are introducing AutoGen Studio, an interface powered by <a href="https://github.com/microsoft/autogen/tree/main/autogen">AutoGen</a>. It allows you to:</p><ul><li>Declaratively define and modify agents and multi-agent workflows through a point and click, drag and drop interface (e.g., you can select the parameters of two agents that will communicate to solve your task).</li><li>Use our UI to create chat sessions with the specified agents and view results (e.g., view chat history, generated files, and time taken).</li><li>Explicitly add skills to your agents and accomplish more tasks.</li><li>Publish your sessions to a local gallery.</li></ul><p>AutoGen Studio is open source <a href="https://github.com/microsoft/autogen/tree/main/samples/apps/autogen-studio">code here</a>, and can be installed via pip.  Give it a try!</p><pre><code class="language-bash">pip install autogenstudio
</code></pre><h2>Introduction</h2><p>The accelerating pace of technology has ushered us into an era where digital assistants (or agents) are becoming integral to our lives. <a href="https://github.com/microsoft/autogen/tree/main/autogen">AutoGen</a> has emerged as a leading framework for orchestrating the power of agents. In the spirit of expanding this frontier and democratizing this capability, we are thrilled to introduce a new user-friendly interface: <strong>AutoGen Studio</strong>.</p><p>With AutoGen Studio, users can rapidly create, manage, and interact with agents that can learn, adapt, and collaborate. As we release this interface into the open-source community, our ambition is not only to enhance productivity but to inspire a level of personalized interaction between humans and agents.</p><blockquote><p><strong>Note</strong>: AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app.</p></blockquote><h2>Getting Started with AutoGen Studio</h2><p>The following guide will help you get AutoGen Studio up and running on your system.</p><h3>Configuring an LLM Provider</h3><p>To get started, you need access to a language model. You can get this set up by following the steps in the AutoGen documentation <a href="https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints">here</a>. Configure your environment with either <code>OPENAI_API_KEY</code> or <code>AZURE_OPENAI_API_KEY</code>.</p><p>For example, in your terminal, you would set the API key like this:</p><pre><code class="language-bash">export OPENAI_API_KEY=&lt;your_api_key&gt;
</code></pre><p>You can also specify the model directly in the agent&#x27;s configuration as shown below.</p><pre><code class="language-python">llm_config = LLMConfig(
    config_list=[{
        &quot;model&quot;: &quot;gpt-4&quot;,
        &quot;api_key&quot;: &quot;&lt;azure_api_key&gt;&quot;,
        &quot;base_url&quot;: &quot;&lt;azure api base url&gt;&quot;,
        &quot;api_type&quot;: &quot;azure&quot;,
        &quot;api_version&quot;: &quot;2023-06-01-preview&quot;
    }],
    temperature=0,
)
</code></pre><h3>Installation</h3><ol><li><p><strong>Install from PyPi</strong></p><p>We recommend using a virtual environment (e.g., conda) to avoid conflicts with existing Python packages. With Python 3.10 or newer active in your virtual environment, use pip to install AutoGen Studio:</p><pre><code class="language-bash">pip install autogenstudio
</code></pre></li><li><p><strong>Install from Source</strong></p><blockquote><p>Note: This approach requires some familiarity with building interfaces in React.</p></blockquote><p>If you prefer to install from source, ensure you have Python 3.10+ and Node.js (version above 14.15.0) installed. Here&#x27;s how you get started:</p><ul><li><p>Clone the AutoGen Studio repository and install its Python dependencies:</p><pre><code class="language-bash">pip install -e .
</code></pre></li><li><p>Navigate to the <code>samples/apps/autogen-studio/frontend</code> directory, install dependencies, and build the UI:</p><pre><code class="language-bash">npm install -g gatsby-cli
npm install --global yarn
cd frontend
yarn install
yarn build
</code></pre></li></ul><p>For Windows users, to build the frontend, you may need alternative commands provided in the <a href="https://github.com/microsoft/autogen/tree/main/samples/apps/autogen-studio">autogen studio readme</a>.</p></li></ol><h3>Running the Application</h3><p>Once installed, run the web UI by entering the following in your terminal:</p><pre><code class="language-bash">autogenstudio ui --port 8081
</code></pre><p>This will start the application on the specified port. Open your web browser and go to <code>http://localhost:8081/</code> to begin using AutoGen Studio.</p><p>Now that you have AutoGen Studio installed and running, you are ready to explore its capabilities, including defining and modifying agent workflows, interacting with agents and sessions, and expanding agent skills.</p><h2>What Can You Do with AutoGen Studio?</h2><p>The AutoGen Studio UI is organized into 3 high level sections - <strong>Build</strong>, <strong>Playground</strong>, and <strong>Gallery</strong>.</p><h3>Build</h3><p><img src="img/autogenstudio_config.png" alt="Specify Agents."/></p><p>This section focuses on defining the properties of agents and agent workflows. It includes the following concepts:</p><p><strong>Skills</strong>: Skills are functions (e.g., Python functions) that describe how to solve a task. In general, a good skill has a descriptive name (e.g. <code>generate_images</code>), extensive docstrings and good defaults (e.g., writing out files to disk for persistence and reuse). You can add new skills to AutoGen Studio via the provided UI. At inference time, these skills are made available to the assistant agent as they address your tasks.</p><p><img src="img/autogenstudio_skills.png" alt="View and add skills."/></p><p align="center"><em>AutoGen Studio Build View: View, add or edit skills that an agent can leverage in addressing tasks.</em></p><p><strong>Agents</strong>: This provides an interface to declaratively specify properties for an AutoGen agent (mirrors most of the members of a base <a href="https://github.com/microsoft/autogen/blob/main/autogen/agentchat/conversable_agent.py">AutoGen conversable agent</a> class).</p><p><strong>Agent Workflows</strong>: An agent workflow is a specification of a set of agents that can work together to accomplish a task. The simplest version of this is a setup with two agents – a user proxy agent (that represents a user i.e. it compiles code and prints result) and an assistant that can address task requests (e.g., generating plans, writing code, evaluating responses, proposing error recovery steps, etc.). A more complex flow could be a group chat where even more agents work towards a solution.</p><h3>Playground</h3><p><img src="img/autogenstudio_home.png" alt="AutoGen Studio Playground View: Solving a task with multiple agents that generate a pdf document with images."/></p><p align="center"><em>AutoGen Studio Playground View: Agents collaborate, use available skills (ability to generate images) to address a user task (generate pdf&#x27;s).</em></p><p>The playground section is focused on interacting with agent workflows defined in the previous build section. It includes the following concepts:</p><p><strong>Session</strong>: A session refers to a period of continuous interaction or engagement with an agent workflow, typically characterized by a sequence of activities or operations aimed at achieving specific objectives. It includes the agent workflow configuration, the interactions between the user and the agents. A session can be “published” to a “gallery”.</p><p><strong>Chat View</strong>: A chat is a sequence of interactions between a user and an agent. It is a part of a session.</p><h3>Gallery</h3><p>This section is focused on sharing and reusing artifacts (e.g., workflow configurations, sessions, etc.).</p><p>AutoGen Studio comes with 3 example skills: <code>fetch_profile</code>, <code>find_papers</code>, <code>generate_images</code>. Please feel free to review the repo to learn more about how they work.</p><h2>The AutoGen Studio API</h2><p>While  AutoGen Studio is a web interface, it is powered by an underlying python API that is reusable and modular. Importantly, we have implemented an API where agent workflows can be declaratively specified (in JSON), loaded and run. An example of the current API is shown below. Please consult the <a href="https://microsoft.github.io/autogen/docs/autogenstudio">AutoGen Studio repo</a> for more details.</p><pre><code class="language-python">import json
from autogenstudio import AutoGenWorkFlowManager, AgentWorkFlowConfig

# load an agent specification in JSON
agent_spec = json.load(open(&#x27;agent_spec.json&#x27;))

# Create an AutoGen Workflow Configuration from the agent specification
agent_work_flow_config = FlowConfig(**agent_spec)

# Create a Workflow from the configuration
agent_work_flow = AutoGenWorkFlowManager(agent_work_flow_config)

# Run the workflow on a task
task_query = &quot;What is the height of the Eiffel Tower?&quot;
agent_work_flow.run(message=task_query)
</code></pre><h2>Road Map and Next Steps</h2><p>As we continue to develop and refine AutoGen Studio, the road map below outlines an array of enhancements and new features planned for future releases. Here&#x27;s what users can look forward to:</p><ul><li><strong>Complex Agent Workflows</strong>: We&#x27;re working on integrating support for more sophisticated agent workflows, such as <code>GroupChat</code>, allowing for richer interaction between multiple agents or dynamic topologies.</li><li><strong>Improved User Experience</strong>: This includes features like streaming intermediate model output for real-time feedback, better summarization of agent responses, information on costs of each interaction. We will also invest in improving the workflow for composing and reusing agents. We will also explore support for more interactive human in the loop feedback to agents.</li><li><strong>Expansion of Agent Skills</strong>: We will work towards improving the workflow for authoring, composing and reusing agent skills.</li><li><strong>Community Features</strong>: Facilitation of sharing and collaboration within AutoGen Studio user community is a key goal. We&#x27;re exploring options for sharing sessions and results more easily among users and contributing to a shared repository of skills, agents, and agent workflows.</li></ul><h3>FAQ</h3><p><strong>Q: Where can I adjust the default skills, agent and workflow configurations?</strong>
A: You can modify agent configurations directly from the UI or by editing the <code>autogentstudio/utils/dbdefaults.json</code> file which is used to initialize the database.</p><p><strong>Q: If I want to reset the entire conversation with an agent, how do I go about it?</strong>
A: To reset your conversation history, you can delete the <code>database.sqlite</code> file. If you need to clear user-specific data, remove the relevant <code>autogenstudio/web/files/user/&lt;user_id_md5hash&gt;</code> folder.</p><p><strong>Q: Is it possible to view the output and messages generated by the agents during interactions?</strong>
A: Yes, you can view the generated messages in the debug console of the web UI, providing insights into the agent interactions. Alternatively, you can inspect the <code>database.sqlite</code> file for a comprehensive record of messages.</p><p><strong>Q: Where can I find documentation and support for AutoGen Studio?</strong>
A: We are constantly working to improve AutoGen Studio. For the latest updates, please refer to the <a href="https://github.com/microsoft/autogen/tree/main/samples/apps/autogen-studio">AutoGen Studio Readme</a>. For additional support, please open an issue on <a href="https://github.com/microsoft/autogen">GitHub</a> or ask questions on <a href="https://discord.gg/pAbnFJrkgZ">Discord</a>.</p><br/>]]></content>
        <author>
            <name>Victor Dibia</name>
            <uri>https://github.com/victordibia</uri>
        </author>
        <author>
            <name>Gagan Bansal</name>
            <uri>https://www.linkedin.com/in/gagan-bansal/</uri>
        </author>
        <author>
            <name>Saleema Amershi</name>
            <uri>https://github.com/samershi</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Agent AutoBuild - Automatically Building Multi-agent Systems]]></title>
        <id>Agent AutoBuild - Automatically Building Multi-agent Systems</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/11/26/Agent-AutoBuild"/>
        <updated>2023-11-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Overall structure of AutoBuild]]></summary>
        <content type="html"><![CDATA[<p><img src="img/agent_autobuild.png" alt="Overall structure of AutoBuild"/></p><p><strong>TL;DR:</strong>
Introducing <strong>AutoBuild</strong>, building multi-agent system automatically, fast, and easily for complex tasks with minimal
user prompt required, powered by a new designed class <strong>AgentBuilder</strong>. AgentBuilder also supports open-source LLMs by
leveraging <a href="https://docs.vllm.ai/en/latest/index.html">vLLM</a> and <a href="https://github.com/lm-sys/FastChat">FastChat</a>.
Checkout example notebooks and source code for reference:</p><ul><li><a href="https://github.com/microsoft/autogen/blob/main/notebook/autobuild_basic.ipynb">AutoBuild Examples</a></li><li><a href="https://github.com/microsoft/autogen/blob/main/autogen/agentchat/contrib/agent_builder.py">AgentBuilder</a></li></ul><h2>Introduction</h2><p>In this blog, we introduce <strong>AutoBuild</strong>, a pipeline that can automatically build multi-agent systems for complex tasks.
Specifically, we design a new class called <strong>AgentBuilder</strong>, which will complete the generation of participant expert agents
and the construction of group chat automatically after the user provides descriptions of a building task and an execution task.</p><p>AgentBuilder supports open-source models on Hugging Face powered by <a href="https://docs.vllm.ai/en/latest/index.html">vLLM</a>
and <a href="https://github.com/lm-sys/FastChat">FastChat</a>. Once the user chooses to use open-source LLM, AgentBuilder will set
up an endpoint server automatically without any user participation.</p><h2>Installation</h2><ul><li>AutoGen:</li></ul><pre><code class="language-bash">pip install pyautogen[autobuild]
</code></pre><ul><li>(Optional: if you want to use open-source LLMs) vLLM and FastChat</li></ul><pre><code class="language-bash">pip install vllm fastchat
</code></pre><h2>Basic Example</h2><p>In this section, we provide a step-by-step example of how to use AgentBuilder to build a multi-agent system for a specific task.</p><h3>Step 1: prepare configurations</h3><p>First, we need to prepare the Agent configurations.
Specifically, a config path containing the model name and API key, and a default config for each agent, are required.</p><pre><code class="language-python">config_file_or_env = &#x27;/home/elpis_ubuntu/LLM/autogen/OAI_CONFIG_LIST&#x27;  # modify path
default_llm_config = {
    &#x27;temperature&#x27;: 0
}
</code></pre><h3>Step 2: create an AgentBuilder instance</h3><p>Then, we create an AgentBuilder instance with the config path and default config.
You can also specific the builder model and agent model, which are the LLMs used for building and agent respectively.</p><pre><code class="language-python">from autogen.agentchat.contrib.agent_builder import AgentBuilder

builder = AgentBuilder(config_file_or_env=config_file_or_env, builder_model=&#x27;gpt-4-1106-preview&#x27;, agent_model=&#x27;gpt-4-1106-preview&#x27;)
</code></pre><h3>Step 3: specify the building task</h3><p>Specify a building task with a general description. Building task will help the build manager (a LLM) decide what agents should be built.
Note that your building task should have a general description of the task. Adding some specific examples is better.</p><pre><code class="language-python">building_task = &quot;Find a paper on arxiv by programming, and analyze its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.&quot;
</code></pre><h3>Step 4: build group chat agents</h3><p>Use <code>build()</code> to let the build manager (with a <code>builder_model</code> as backbone) complete the group chat agents generation.
If you think coding is necessary for your task, you can use <code>coding=True</code> to add a user proxy (a local code interpreter) into the agent list as:</p><pre><code class="language-python">agent_list, agent_configs = builder.build(building_task, default_llm_config, coding=True)
</code></pre><p>If <code>coding</code> is not specified, AgentBuilder will determine on its own whether the user proxy should be added or not according to the task.
The generated <code>agent_list</code> is a list of <code>AssistantAgent</code> instances.
If <code>coding</code> is true, a user proxy (a <code>UserProxyAssistant</code> instance) will be added as the first element to the <code>agent_list</code>.
<code>agent_configs</code> is a list of agent configurations including agent name, backbone LLM model, and system message.
For example</p><pre><code>// an example of agent_configs. AgentBuilder will generate agents with the following configurations.
[
    {
        &quot;name&quot;: &quot;ArXiv_Data_Scraper_Developer&quot;,
        &quot;model&quot;: &quot;gpt-4-1106-preview&quot;,
        &quot;system_message&quot;: &quot;You are now in a group chat. You need to complete a task with other participants. As an ArXiv_Data_Scraper_Developer, your focus is to create and refine tools capable of intelligent search and data extraction from arXiv, honing in on topics within the realms of computer science and medical science. Utilize your proficiency in Python programming to design scripts that navigate, query, and parse information from the platform, generating valuable insights and datasets for analysis. \n\nDuring your mission, it\u2019s not just about formulating queries; your role encompasses the optimization and precision of the data retrieval process, ensuring relevance and accuracy of the information extracted. If you encounter an issue with a script or a discrepancy in the expected output, you are encouraged to troubleshoot and offer revisions to the code you find in the group chat.\n\nWhen you reach a point where the existing codebase does not fulfill task requirements or if the operation of provided code is unclear, you should ask for help from the group chat manager. They will facilitate your advancement by providing guidance or appointing another participant to assist you. Your ability to adapt and enhance scripts based on peer feedback is critical, as the dynamic nature of data scraping demands ongoing refinement of techniques and approaches.\n\nWrap up your participation by confirming the user&#x27;s need has been satisfied with the data scraping solutions you&#x27;ve provided. Indicate the completion of your task by replying \&quot;TERMINATE\&quot; in the group chat.&quot;,
        &quot;description&quot;: &quot;ArXiv_Data_Scraper_Developer is a specialized software development role requiring proficiency in Python, including familiarity with web scraping libraries such as BeautifulSoup or Scrapy, and a solid understanding of APIs and data parsing. They must possess the ability to identify and correct errors in existing scripts and confidently engage in technical discussions to improve data retrieval processes. The role also involves a critical eye for troubleshooting and optimizing code to ensure efficient data extraction from the ArXiv platform for research and analysis purposes.&quot;
    },
    ...
]
</code></pre><h3>Step 5: execute the task</h3><p>Let agents generated in <code>build()</code> complete the task collaboratively in a group chat.</p><pre><code class="language-python">import autogen

def start_task(execution_task: str, agent_list: list, llm_config: dict):
    config_list = autogen.config_list_from_json(config_file_or_env, filter_dict={&quot;model&quot;: [&quot;gpt-4-1106-preview&quot;]})

    group_chat = autogen.GroupChat(agents=agent_list, messages=[], max_round=12)
    manager = autogen.GroupChatManager(
        groupchat=group_chat, llm_config={&quot;config_list&quot;: config_list, **llm_config}
    )
    agent_list[0].initiate_chat(manager, message=execution_task)

start_task(
    execution_task=&quot;Find a recent paper about gpt-4 on arxiv and find its potential applications in software.&quot;,
    agent_list=agent_list,
    llm_config=default_llm_config
)
</code></pre><h3>Step 6 (Optional): clear all agents and prepare for the next task</h3><p>You can clear all agents generated in this task by the following code if your task is completed or if the next task is largely different from the current task.</p><pre><code class="language-python">builder.clear_all_agents(recycle_endpoint=True)
</code></pre><p>If the agent&#x27;s backbone is an open-source LLM, this process will also shut down the endpoint server. More details are in the next section.
If necessary, you can use <code>recycle_endpoint=False</code> to retain the previous open-source LLM&#x27;s endpoint server.</p><h2>Save and Load</h2><p>You can save all necessary information of the built group chat agents by</p><pre><code class="language-python">saved_path = builder.save()
</code></pre><p>Configurations will be saved in JSON format with the following content:</p><pre><code class="language-json">// FILENAME: save_config_TASK_MD5.json
{
    &quot;building_task&quot;: &quot;Find a paper on arxiv by programming, and analysis its application in some domain. For example, find a latest paper about gpt-4 on arxiv and find its potential applications in software.&quot;,
    &quot;agent_configs&quot;: [
        {
            &quot;name&quot;: &quot;...&quot;,
            &quot;model&quot;: &quot;...&quot;,
            &quot;system_message&quot;: &quot;...&quot;,
            &quot;description&quot;: &quot;...&quot;
        },
        ...
    ],
    &quot;manager_system_message&quot;: &quot;...&quot;,
    &quot;code_execution_config&quot;: {...},
    &quot;default_llm_config&quot;: {...}
}
</code></pre><p>You can provide a specific filename, otherwise, AgentBuilder will save config to the current path with the generated filename <code>save_config_TASK_MD5.json</code>.</p><p>You can load the saved config and skip the building process. AgentBuilder will create agents with those information without prompting the build manager.</p><pre><code class="language-python">new_builder = AgentBuilder(config_file_or_env=config_file_or_env)
agent_list, agent_config = new_builder.load(saved_path)
start_task(...)  # skip build()
</code></pre><h2>Use OpenAI Assistant</h2><p><a href="https://platform.openai.com/docs/assistants/overview">Assistants API</a> allows you to build AI assistants within your own applications.
An Assistant has instructions and can leverage models, tools, and knowledge to respond to user queries.
AutoBuild also supports the assistant API by adding <code>use_oai_assistant=True</code> to <code>build()</code>.</p><pre><code class="language-python"># Transfer to the OpenAI Assistant API.
agent_list, agent_config = new_builder.build(building_task, default_llm_config, use_oai_assistant=True)
...
</code></pre><h2>(Experimental) Use Open-source LLM</h2><p>AutoBuild supports open-source LLM by <a href="https://docs.vllm.ai/en/latest/index.html">vLLM</a> and <a href="https://github.com/lm-sys/FastChat">FastChat</a>.
Check the supported model list <a href="https://docs.vllm.ai/en/latest/models/supported_models.html">here</a>.
After satisfying the requirements, you can add an open-source LLM&#x27;s huggingface repository to the config file,</p><pre><code class="language-json,">// Add the LLM&#x27;s huggingface repo to your config file and use EMPTY as the api_key.
[
    ...
    {
        &quot;model&quot;: &quot;meta-llama/Llama-2-13b-chat-hf&quot;,
        &quot;api_key&quot;: &quot;EMPTY&quot;
    }
]
</code></pre><p>and specify it when initializing AgentBuilder.
AgentBuilder will automatically set up an endpoint server for open-source LLM. Make sure you have sufficient GPUs resources.</p><h2>Future work/Roadmap</h2><ul><li>Let the builder select the best agents from a given library/database to solve the task.</li></ul><h2>Summary</h2><p>We propose AutoBuild with a new class <code>AgentBuilder</code>.
AutoBuild can help user solve their complex task with an automatically built multi-agent system.
AutoBuild supports open-source LLMs and GPTs API, giving users more flexibility to choose their favorite models.
More advanced features are coming soon.</p>]]></content>
        <author>
            <name>Linxin Song</name>
            <uri>https://linxins97.github.io/</uri>
        </author>
        <author>
            <name>Jieyu Zhang</name>
            <uri>https://jieyuz2.github.io/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Assess Utility of LLM-powered Applications?]]></title>
        <id>How to Assess Utility of LLM-powered Applications?</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/11/20/AgentEval"/>
        <updated>2023-11-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Fig.1: A verification framework]]></summary>
        <content type="html"><![CDATA[<p><img src="img/agenteval-CQ.png" alt="Fig.1: A verification framework"/></p><p align="center"><em>Fig.1 illustrates the general flow of AgentEval</em></p><p><strong>TL;DR:</strong></p><ul><li>As a developer of an LLM-powered application, how can you assess the utility it brings to end users while helping them with their tasks?</li><li>To shed light on the question above, we introduce <code>AgentEval</code> — the first version of the framework to assess the utility of any LLM-powered application crafted to assist users in specific tasks.  AgentEval aims to simplify the evaluation process by automatically proposing a set of criteria tailored to the unique purpose of your application. This allows for a comprehensive assessment, quantifying the utility of your application against the suggested criteria.</li><li>We demonstrate how <code>AgentEval</code> work using <a href="https://microsoft.github.io/autogen/blog/2023/06/28/MathChat">math problems dataset</a> as an example in the <a href="https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb">following notebook</a>. Any feedback would be useful for future development. Please contact us on our <a href="http://aka.ms/autogen-dc">Discord</a>.</li></ul><h2>Introduction</h2><p> AutoGen aims to simplify the development of LLM-powered multi-agent systems for various applications, ultimately making end users&#x27; lives easier by assisting with their tasks. Next, we all yearn to understand how our developed systems perform, their utility for users, and, perhaps most crucially, how we can enhance them. Directly evaluating multi-agent systems poses challenges as current approaches predominantly rely on success metrics – essentially, whether the agent accomplishes tasks. However, comprehending user interaction with a system involves far more than success alone. Take math problems, for instance; it&#x27;s not merely about the agent solving the problem. Equally significant is its ability to convey solutions based on various criteria, including completeness, conciseness, and the clarity of the provided explanation. Furthermore, success isn&#x27;t always clearly defined for every task.</p><p> Rapid advances in LLMs and multi-agent systems have brought forth many emerging capabilities that we&#x27;re keen on translating into tangible utilities for end users. We introduce the first version of <code>AgentEval</code> framework - a tool crafted to empower developers in swiftly gauging the utility of LLM-powered applications designed to help end users accomplish the desired task.</p><p><img src="img/tasks-taxonomy.png" alt="Fig.2: An overview of the tasks taxonomy"/></p><p align="center"><em>Fig. 2 provides  an overview of the tasks taxonomy</em></p><p>Let&#x27;s first look into an overview of the suggested task taxonomy that a multi-agent system can be designed for. In general, the tasks can be split into two types, where:</p><ul><li><em>Success is not clearly defined</em> - refer to instances when users utilize a system in an assistive manner, seeking suggestions rather than expecting the system to solve the task. For example, a user might request the system to generate an email. In many cases, this generated content serves as a template that the user will later edit. However, defining success precisely for such tasks is relatively complex.</li><li><em>Success is clearly defined</em> - refer to instances where we can clearly define whether a system solved the task or not. Consider agents that assist in accomplishing household tasks, where the definition of success is clear and measurable. This category can be further divided into two separate subcategories:<ul><li><em>The optimal solution exits</em> - these are tasks where only one solution is possible. For example, if you ask your assistant to turn on the light, the success of this task is clearly defined, and there is only one way to accomplish it.</li><li><em>Multiple solutions exist</em> - increasingly, we observe situations where multiple trajectories of agent behavior can lead to either success or failure. In such cases, it is crucial to differentiate between the various successful and unsuccessful trajectories. For example, when you ask the agent to suggest you a food recipe or tell you a joke.</li></ul></li></ul><p>In our <code>AgentEval</code> framework, we are currently focusing on tasks where <em>Success is clearly defined</em>. Next, we will introduce the suggested framework.</p><h2><code>AgentEval</code> Framework</h2><p>Our previous research on <a href="https://github.com/microsoft/iglu-datasets">assistive agents in Minecraft</a> suggested that the most optimal way to obtain human judgments is to present humans with two agents side by side and ask for preferences. In this setup of pairwise comparison, humans can develop criteria to explain why they prefer the behavior of one agent over another. For instance, <em>&#x27;the first agent was faster in execution,&#x27;</em> or <em>&#x27;the second agent moves more naturally.&#x27;</em> So, the comparative nature led humans to come up with a list of criteria that helps to infer the utility of the task. With this idea in mind, we designed <code>AgentEval</code> (shown in Fig. 1), where we employ LLMs to help us understand, verify, and assess task <em>utility</em> for the multi-agent system. Namely:</p><ul><li>The goal of <code>CriticAgent</code> is to suggest the list of criteria (Fig. 1), that can be used to assess task utility. This is an example of how <code>CriticAgent</code> is defined using <code>Autogen</code>:</li></ul><pre><code class="language-python">critic = autogen.AssistantAgent(
    name=&quot;critic&quot;,
    llm_config={&quot;config_list&quot;: config_list},
    system_message=&quot;&quot;&quot;You are a helpful assistant. You suggest criteria for evaluating different tasks. They should be distinguishable, quantifiable, and not redundant.
    Convert the evaluation criteria into a dictionary where the keys are the criteria.
    The value of each key is a dictionary as follows {&quot;description&quot;: criteria description, &quot;accepted_values&quot;: possible accepted inputs for this key}
    Make sure the keys are criteria for assessing the given task. &quot;accepted_values&quot; include the acceptable inputs for each key that are fine-grained and preferably multi-graded levels. &quot;description&quot; includes the criterion description.
    Return only the dictionary.&quot;&quot;&quot;
)
</code></pre><p>Next, the critic is given successful and failed examples of the task execution; then, it is able to return a list of criteria (Fig. 1). For reference, use the <a href="https://github.com/microsoft/autogen/blob/main/notebook/agenteval_cq_math.ipynb">following notebook</a>.</p><ul><li>The goal of <code>QuantifierAgent</code> is to quantify each of the suggested criteria (Fig. 1), providing us with an idea of the utility of this system for the given task. Here is an example of how it can be defined:</li></ul><pre><code class="language-python">quantifier = autogen.AssistantAgent(
    name=&quot;quantifier&quot;,
    llm_config={&quot;config_list&quot;: config_list},
    system_message = &quot;&quot;&quot;You are a helpful assistant. You quantify the output of different tasks based on the given criteria.
    The criterion is given in a dictionary format where each key is a distinct criteria.
    The value of each key is a dictionary as follows {&quot;description&quot;: criteria description , &quot;accepted_values&quot;: possible accepted inputs for this key}
    You are going to quantify each of the criteria for a given task based on the task description.
    Return a dictionary where the keys are the criteria and the values are the assessed performance based on accepted values for each criteria.
    Return only the dictionary.&quot;&quot;&quot;

)
</code></pre><h2><code>AgentEval</code> Results based on Math Problems Dataset</h2><p> As an example, after running CriticAgent, we obtained the following criteria to verify the results for math problem dataset:</p><table><thead><tr><th>Criteria</th><th>Description</th><th>Accepted Values</th></tr></thead><tbody><tr><td>Problem Interpretation</td><td>Ability to correctly interpret the problem</td><td>[&quot;completely off&quot;, &quot;slightly relevant&quot;, &quot;relevant&quot;, &quot;mostly accurate&quot;, &quot;completely accurate&quot;]</td></tr><tr><td>Mathematical Methodology</td><td>Adequacy of the chosen mathematical or algorithmic methodology for the question</td><td>[&quot;inappropriate&quot;, &quot;barely adequate&quot;, &quot;adequate&quot;, &quot;mostly effective&quot;, &quot;completely effective&quot;]</td></tr><tr><td>Calculation Correctness</td><td>Accuracy of calculations made and solutions given</td><td>[&quot;completely incorrect&quot;, &quot;mostly incorrect&quot;, &quot;neither&quot;, &quot;mostly correct&quot;, &quot;completely correct&quot;]</td></tr><tr><td>Explanation Clarity</td><td>Clarity and comprehensibility of explanations, including language use and structure</td><td>[&quot;not at all clear&quot;, &quot;slightly clear&quot;, &quot;moderately clear&quot;, &quot;very clear&quot;, &quot;completely clear&quot;]</td></tr><tr><td>Code Efficiency</td><td>Quality of code in terms of efficiency and elegance</td><td>[&quot;not at all efficient&quot;, &quot;slightly efficient&quot;, &quot;moderately efficient&quot;, &quot;very efficient&quot;, &quot;extremely efficient&quot;]</td></tr><tr><td>Code Correctness</td><td>Correctness of the provided code</td><td>[&quot;completely incorrect&quot;, &quot;mostly incorrect&quot;, &quot;partly correct&quot;, &quot;mostly correct&quot;, &quot;completely correct&quot;]</td></tr></tbody></table><p>Then, after running QuantifierAgent, we obtained the results presented in Fig. 3, where you can see three models:</p><ul><li>AgentChat</li><li>ReAct</li><li>GPT-4 Vanilla Solver</li></ul><p>Lighter colors represent estimates for failed cases, and brighter colors show how discovered criteria were quantified.</p><p><img src="img/math-problems-plot.png" alt="Fig.3: Results based on overall math problems dataset `_s` stands for successful cases, `_f` - stands for failed cases"/></p><p align="center"><em>Fig.3 presents results based on overall math problems dataset `_s` stands for successful cases, `_f` - stands for failed cases</em></p><p>We note that while applying agentEval to math problems, the agent was not exposed to any ground truth information about the problem. As such, this figure illustrates an estimated performance of the three different agents, namely, Autogen (blue), Gpt-4 (red), and ReAct (green). We observe that by comparing the performance of any of the three agents in successful cases (dark bars of any color) versus unsuccessful cases (lighter version of the same bar), we note that AgentEval was able to assign higher quantification to successful cases than that of failed ones. This observation verifies AgentEval&#x27;s ability for task utility prediction. Additionally, AgentEval allows us to go beyond just a binary definition of success, enabling a more in-depth comparison between successful and failed cases.</p><p>It&#x27;s important not only to identify what is not working but also to recognize what and why actually went well.</p><h2>Limitations and Future Work</h2><p>The current implementation of <code>AgentEval</code> has a number of limitations which are planning to overcome in the future:</p><ul><li>The list of criteria varies per run (unless you store a seed). We would recommend to run <code>CriticAgent</code> at least two times, and pick criteria you think is important for your domain.</li><li>The results of the <code>QuantifierAgent</code> can vary with each run, so we recommend conducting multiple runs to observe the extent of result variations.</li></ul><p>To mitigate the limitations mentioned above, we are working on VerifierAgent, whose goal is to stabilize the results and provide additional explanations.</p><h2>Summary</h2><p><code>CriticAgent</code> and <code>QuantifierAgent</code> can be applied to the logs of any type of application, providing you with an in-depth understanding of the utility your solution brings to the user for a given task.</p><p>We would love to hear about how AgentEval works for your application. Any feedback would be useful for future development. Please contact us on our <a href="http://aka.ms/autogen-dc">Discord</a>.</p><h2>Previous Research</h2><pre><code>@InProceedings{pmlr-v176-kiseleva22a,
  title = &quot;Interactive Grounded Language Understanding in a Collaborative Environment: IGLU 2021&quot;,
  author = &quot;Kiseleva, Julia and Li, Ziming and Aliannejadi, Mohammad and Mohanty, Shrestha and ter Hoeve, Maartje and Burtsev, Mikhail and Skrynnik, Alexey and Zholus, Artem and Panov, Aleksandr and Srinet, Kavya and Szlam, Arthur and Sun, Yuxuan and Hofmann, Katja and C{\^o}t{\&#x27;e}, Marc-Alexandre and Awadallah, Ahmed and Abdrazakov, Linar and Churin, Igor and Manggala, Putra and Naszadi, Kata and van der Meer, Michiel and Kim, Taewoon&quot;,
  booktitle = &quot;Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track&quot;,
  pages = &quot;146--161&quot;,
  year = 2022,
  editor = &quot;Kiela, Douwe and Ciccone, Marco and Caputo, Barbara&quot;,
  volume = 176,
  series = &quot;Proceedings of Machine Learning Research&quot;,
  month = &quot;06--14 Dec&quot;,
  publisher = &quot;PMLR&quot;,
  pdf =      {https://proceedings.mlr.press/v176/kiseleva22a/kiseleva22a.pdf},
  url =      {https://proceedings.mlr.press/v176/kiseleva22a.html}.
}
</code></pre><pre><code>@InProceedings{pmlr-v220-kiseleva22a,
  title = &quot;Interactive Grounded Language Understanding in a Collaborative Environment: Retrospective on Iglu 2022 Competition&quot;,
  author = &quot;Kiseleva, Julia and Skrynnik, Alexey and Zholus, Artem and Mohanty, Shrestha and Arabzadeh, Negar and C\^{o}t\&#x27;e, Marc-Alexandre and Aliannejadi, Mohammad and Teruel, Milagro and Li, Ziming and Burtsev, Mikhail and ter Hoeve, Maartje and Volovikova, Zoya and Panov, Aleksandr and Sun, Yuxuan and Srinet, Kavya and Szlam, Arthur and Awadallah, Ahmed and Rho, Seungeun and Kwon, Taehwan and Wontae Nam, Daniel and Bivort Haiek, Felipe and Zhang, Edwin and Abdrazakov, Linar and Qingyam, Guo and Zhang, Jason and Guo, Zhibin&quot;,
  booktitle = &quot;Proceedings of the NeurIPS 2022 Competitions Track&quot;,
  pages = &quot;204--216&quot;,
  year = 2022,
  editor = &quot;Ciccone, Marco and Stolovitzky, Gustavo and Albrecht, Jacob&quot;,
  volume = 220,
  series = &quot;Proceedings of Machine Learning Research&quot;,
  month = &quot;28 Nov--09 Dec&quot;,
  publisher = &quot;PMLR&quot;,
  pdf = &quot;https://proceedings.mlr.press/v220/kiseleva22a/kiseleva22a.pdf&quot;,
  url = &quot;https://proceedings.mlr.press/v220/kiseleva22a.html&quot;.
}
</code></pre>]]></content>
        <author>
            <name>Julia Kiseleva</name>
            <uri>https://github.com/julianakiseleva/</uri>
        </author>
        <author>
            <name>Negar Arabzadeh</name>
            <uri>https://www.negara.me/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoGen Meets GPTs]]></title>
        <id>AutoGen Meets GPTs</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants"/>
        <updated>2023-11-13T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[OpenAI Assistant]]></summary>
        <content type="html"><![CDATA[<p><img src="img/teaser.jpg" alt="OpenAI Assistant"/></p><p align="center"><em>AutoGen enables collaboration among multiple ChatGPTs for complex tasks.</em></p><h2>TLDR</h2><p>OpenAI assistants are now integrated into AutoGen via <a href="https://github.com/microsoft/autogen/blob/main/autogen/agentchat/contrib/gpt_assistant_agent.py"><code>GPTAssistantAgent</code></a>.
This enables multiple OpenAI assistants, which form the backend of the now popular GPTs, to collaborate and tackle complex tasks.
Checkout example notebooks for reference:</p><ul><li><a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_twoagents_basic.ipynb">Basic example</a></li><li><a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_code_interpreter.ipynb">Code interpreter</a></li><li><a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_oai_assistant_function_call.ipynb">Function calls</a></li></ul><h2>Introduction</h2><p>Earlier last week, OpenAI introduced <a href="https://openai.com/blog/introducing-gpts">GPTs</a>, giving users ability to create custom ChatGPTs tailored for them.
<em>But what if these individual GPTs could collaborate to do even more?</em>
Fortunately, because of AutoGen, this is now a reality!
AutoGen has been pioneering agents and supporting <a href="https://aka.ms/autogen-pdf">multi-agent workflows</a> since earlier this year, and now (starting with version 0.2.0b5) we are introducing compatibility with the <a href="https://openai.com/blog/introducing-gpts">Assistant API</a>, which is currently in beta preview.</p><p>To accomplish this, we&#x27;ve added a new (experimental) agent called the <code>GPTAssistantAgent</code> that
lets you seamlessly add these new OpenAI assistants into AutoGen-based multi-agent workflows.
This integration shows great potential and synergy, and we plan to continue enhancing it.</p><h2>Installation</h2><pre><code class="language-bash">pip install pyautogen==0.2.0b5
</code></pre><h2>Basic Example</h2><p>Here&#x27;s a basic example that uses a <code>UserProxyAgent</code> to allow an interface
with the <code>GPTAssistantAgent</code>.</p><p>First, import the new agent and setup <code>config_list</code>:</p><pre><code class="language-python">from autogen import config_list_from_json
from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent
from autogen import UserProxyAgent

config_list = config_list_from_json(&quot;OAI_CONFIG_LIST&quot;)
</code></pre><p>Then simply define the OpenAI assistant agent and give it the task!</p><pre><code class="language-python"># creates new assistant using Assistant API
gpt_assistant = GPTAssistantAgent(
    name=&quot;assistant&quot;,
    llm_config={
        &quot;config_list&quot;: config_list,
        &quot;assistant_id&quot;: None
    })

user_proxy = UserProxyAgent(name=&quot;user_proxy&quot;,
    code_execution_config={
        &quot;work_dir&quot;: &quot;coding&quot;
    },
    human_input_mode=&quot;NEVER&quot;)

user_proxy.initiate_chat(gpt_assistant, message=&quot;Print hello world&quot;)
</code></pre><p><code>GPTAssistantAgent</code> supports both creating new OpenAI assistants or reusing existing assistants
(e.g, by providing an <code>assistant_id</code>).</p><h2>Code Interpreter Example</h2><p><code>GPTAssistantAgent</code> allows you to specify an OpenAI tools
(e.g., function calls, code interpreter, etc). The example below enables an assistant
that can use OpenAI code interpreter to solve tasks.</p><pre><code class="language-python"># creates new assistant using Assistant API
gpt_assistant = GPTAssistantAgent(
    name=&quot;assistant&quot;,
    llm_config={
        &quot;config_list&quot;: config_list,
        &quot;assistant_id&quot;: None,
        &quot;tools&quot;: [
            {
                &quot;type&quot;: &quot;code_interpreter&quot;
            }
        ],
    })

user_proxy = UserProxyAgent(name=&quot;user_proxy&quot;,
    code_execution_config={
        &quot;work_dir&quot;: &quot;coding&quot;
    },
    human_input_mode=&quot;NEVER&quot;)

user_proxy.initiate_chat(gpt_assistant, message=&quot;Print hello world&quot;)
</code></pre><p>Checkout more examples <a href="https://github.com/microsoft/autogen/tree/main/notebook">here</a>.</p><h2>Limitations and Future Work</h2><ul><li>Group chat managers using GPT assistant are pending.</li><li>GPT assistants with multimodal capabilities haven&#x27;t been released yet but we are committed to support them.</li></ul><h2>Acknowledgements</h2><p><code>GPTAssistantAgent</code> was made possible through collaboration with
<a href="https://github.com/IANTHEREAL">@IANTHEREAL</a>,
<a href="https://leoljl.github.io">Jiale Liu</a>,
<a href="https://github.com/kevin666aa">Yiran Wu</a>,
<a href="https://qingyun-wu.github.io/">Qingyun Wu</a>,
<a href="https://www.microsoft.com/en-us/research/people/chiw/">Chi Wang</a>, and many other AutoGen maintainers.</p>]]></content>
        <author>
            <name>Gagan Bansal</name>
            <uri>https://www.linkedin.com/in/gagan-bansal/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EcoAssistant - Using LLM Assistants More Accurately and Affordably]]></title>
        <id>EcoAssistant - Using LLM Assistants More Accurately and Affordably</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/11/09/EcoAssistant"/>
        <updated>2023-11-09T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[system]]></summary>
        <content type="html"><![CDATA[<p><img src="img/system.png" alt="system"/></p><p><strong>TL;DR:</strong></p><ul><li>Introducing the <strong>EcoAssistant</strong>, which is designed to solve user queries more accurately and affordably.</li><li>We show how to let the LLM assistant agent leverage external API to solve user query.</li><li>We show how to reduce the cost of using GPT models via <strong>Assistant Hierarchy</strong>.</li><li>We show how to leverage the idea of Retrieval-augmented Generation (RAG) to improve the success rate via <strong>Solution Demonstration</strong>.</li></ul><h2>EcoAssistant</h2><p>In this blog, we introduce the <strong>EcoAssistant</strong>, a system built upon AutoGen with the goal of solving user queries more accurately and affordably.</p><h3>Problem setup</h3><p>Recently, users have been using conversational LLMs such as ChatGPT for various queries.
Reports indicate that 23% of ChatGPT user queries are for knowledge extraction purposes.
Many of these queries require knowledge that is external to the information stored within any pre-trained large language models (LLMs).
These tasks can only be completed by generating code to fetch necessary information via external APIs that contain the requested information.
In the table below, we show three types of user queries that we aim to address in this work.</p><table><thead><tr><th>Dataset</th><th>API</th><th>Example query</th></tr></thead><tbody><tr><td>Places</td><td><a href="https://developers.google.com/maps/documentation/places/web-service/overview">Google Places</a></td><td>I’m looking for a 24-hour pharmacy in Montreal, can you find one for me?</td></tr><tr><td>Weather</td><td><a href="https://www.weatherapi.com">Weather API</a></td><td>What is the current cloud coverage in Mumbai, India?</td></tr><tr><td>Stock</td><td><a href="https://www.alphavantage.co/documentation/">Alpha Vantage Stock API</a></td><td>Can you give me the opening price of Microsoft for the month of January 2023?</td></tr></tbody></table><h3>Leveraging external APIs</h3><p>To address these queries, we first build a <strong>two-agent system</strong> based on AutoGen,
where the first agent is a <strong>LLM assistant agent</strong> (<code>AssistantAgent</code> in AutoGen) that is responsible for proposing and refining the code and
the second agent is a <strong>code executor agent</strong> (<code>UserProxyAgent</code> in AutoGen) that would extract the generated code and execute it, forwarding the output back to the LLM assistant agent.
A visualization of the two-agent system is shown below.</p><p><img src="img/chat.png" alt="chat"/></p><p>To instruct the assistant agent to leverage external APIs, we only need to add the API name/key dictionary at the beginning of the initial message.
The template is shown below, where the red part is the information of APIs and black part is user query.</p><p><img src="img/template.png" alt="template"/></p><p>Importantly, we don&#x27;t want to reveal our real API key to the assistant agent for safety concerns.
Therefore, we use a <strong>fake API key</strong> to replace the real API key in the initial message.
In particular, we generate a random token (e.g., <code>181dbb37</code>) for each API key and replace the real API key with the token in the initial message.
Then, when the code executor execute the code, the fake API key would be automatically replaced by the real API key.</p><h3>Solution Demonstration</h3><p>In most practical scenarios, queries from users would appear sequentially over time.
Our <strong>EcoAssistant</strong> leverages past success to help the LLM assistants address future queries via <strong>Solution Demonstration</strong>.
Specifically, whenever a query is deemed successfully resolved by user feedback, we capture and store the query and the final generated code snippet.
These query-code pairs are saved in a specialized vector database. When new queries appear, <strong>EcoAssistant</strong> retrieves the most similar query from the database, which is then appended with the associated code to the initial prompt for the new query, serving as a demonstration.
The new template of initial message is shown below, where the blue part corresponds to the solution demonstration.</p><p><img src="img/template-demo.png" alt="template"/></p><p>We found that this utilization of past successful query-code pairs improves the query resolution process with fewer iterations and enhances the system&#x27;s performance.</p><h3>Assistant Hierarchy</h3><p>LLMs usually have different prices and performance, for example, GPT-3.5-turbo is much cheaper than GPT-4 but also less accurate.
Thus, we propose the <strong>Assistant Hierarchy</strong> to reduce the cost of using LLMs.
The core idea is that we use the cheaper LLMs first and only use the more expensive LLMs when necessary.
By this way, we are able to reduce the reliance on expensive LLMs and thus reduce the cost.
In particular, given multiple LLMs, we initiate one assistant agent for each and start the conversation with the most cost-effective LLM assistant.
If the conversation between the current LLM assistant and the code executor concludes without successfully resolving the query, <strong>EcoAssistant</strong> would then restart the conversation with the next more expensive LLM assistant in the hierarchy.
We found that this strategy significantly reduces costs while still effectively addressing queries.</p><h3>A Synergistic Effect</h3><p>We found that the <strong>Assistant Hierarchy</strong> and <strong>Solution Demonstration</strong> of <strong>EcoAssistant</strong> have a synergistic effect.
Because the query-code database is shared by all LLM assistants, even without specialized design,
the solution from more powerful LLM assistant (e.g., GPT-4) could be later retrieved to guide weaker LLM assistant (e.g., GPT-3.5-turbo).
Such a synergistic effect further improves the performance and reduces the cost of <strong>EcoAssistant</strong>.</p><h3>Experimental Results</h3><p>We evaluate <strong>EcoAssistant</strong> on three datasets: Places, Weather, and Stock. When comparing it with a single GPT-4 assistant, we found that <strong>EcoAssistant</strong> achieves a higher success rate with a lower cost as shown in the figure below.
For more details about the experimental results and other experiments, please refer to our <a href="https://arxiv.org/abs/2310.03046">paper</a>.</p><p><img src="img/results.png" alt="exp"/></p><h2>Further reading</h2><p>Please refer to our <a href="https://arxiv.org/abs/2310.03046">paper</a> and <a href="https://github.com/JieyuZ2/EcoAssistant">codebase</a> for more details about <strong>EcoAssistant</strong>.</p><p>If you find this blog useful, please consider citing:</p><pre><code class="language-bibtex">@article{zhang2023ecoassistant,
  title={EcoAssistant: Using LLM Assistant More Affordably and Accurately},
  author={Zhang, Jieyu and Krishna, Ranjay and Awadallah, Ahmed H and Wang, Chi},
  journal={arXiv preprint arXiv:2310.03046},
  year={2023}
}
</code></pre>]]></content>
        <author>
            <name>Jieyu Zhang</name>
            <uri>https://jieyuz2.github.io/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal with GPT-4V and LLaVA]]></title>
        <id>Multimodal with GPT-4V and LLaVA</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/11/06/LMM-Agent"/>
        <updated>2023-11-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[LMM Teaser]]></summary>
        <content type="html"><![CDATA[<p><img src="img/teaser.png" alt="LMM Teaser"/></p><p><strong>In Brief:</strong></p><ul><li>Introducing the <strong>Multimodal Conversable Agent</strong> and the <strong>LLaVA Agent</strong> to enhance LMM functionalities.</li><li>Users can input text and images simultaneously using the <code>&lt;img img_path&gt;</code> tag to specify image loading.</li><li>Demonstrated through the <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb">GPT-4V notebook</a>.</li><li>Demonstrated through the <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb">LLaVA notebook</a>.</li></ul><h2>Introduction</h2><p>Large multimodal models (LMMs) augment large language models (LLMs) with the ability to process multi-sensory data.</p><p>This blog post and the latest AutoGen update concentrate on visual comprehension. Users can input images, pose questions about them, and receive text-based responses from these LMMs.
We support the <code>gpt-4-vision-preview</code> model from OpenAI and <code>LLaVA</code> model from Microsoft now.</p><p>Here, we emphasize the <strong>Multimodal Conversable Agent</strong> and the <strong>LLaVA Agent</strong> due to their growing popularity.
GPT-4V represents the forefront in image comprehension, while LLaVA is an efficient model, fine-tuned from LLama-2.</p><h2>Installation</h2><p>Incorporate the <code>lmm</code> feature during AutoGen installation:</p><pre><code class="language-bash">pip install &quot;pyautogen[lmm]&quot;
</code></pre><p>Subsequently, import the <strong>Multimodal Conversable Agent</strong> or <strong>LLaVA Agent</strong> from AutoGen:</p><pre><code class="language-python">from autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent  # for GPT-4V
from autogen.agentchat.contrib.llava_agent import LLaVAAgent  # for LLaVA
</code></pre><h2>Usage</h2><p>A simple syntax has been defined to incorporate both messages and images within a single string.</p><p>Example of an in-context learning prompt:</p><pre><code class="language-python">prompt = &quot;&quot;&quot;You are now an image classifier for facial expressions. Here are
some examples.

&lt;img happy.jpg&gt; depicts a happy expression.
&lt;img http://some_location.com/sad.jpg&gt; represents a sad expression.
&lt;img obama.jpg&gt; portrays a neutral expression.

Now, identify the facial expression of this individual: &lt;img unknown.png&gt;
&quot;&quot;&quot;

agent = MultimodalConversableAgent()
user = UserProxyAgent()
user.initiate_chat(agent, message=prompt)
</code></pre><p>The <code>MultimodalConversableAgent</code> interprets the input prompt, extracting images from local or internet sources.</p><h2>Advanced Usage</h2><p>Similar to other AutoGen agents, multimodal agents support multi-round dialogues with other agents, code generation, factual queries, and management via a GroupChat interface.</p><p>For example, the <code>FigureCreator</code> in our <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb">GPT-4V notebook</a> and <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_llava.ipynb">LLaVA notebook</a> integrates two agents: a coder (an AssistantAgent) and critics (a multimodal agent).
The coder drafts Python code for visualizations, while the critics provide insights for enhancement. Collaboratively, these agents aim to refine visual outputs.
With <code>human_input_mode=ALWAYS</code>, you can also contribute suggestions for better visualizations.</p><h2>Reference</h2><ul><li><a href="https://openai.com/research/gpt-4v-system-card">GPT-4V System Card</a></li><li><a href="https://github.com/haotian-liu/LLaVA">LLaVA GitHub</a></li></ul><h2>Future Enhancements</h2><p>For further inquiries or suggestions, please open an issue in the <a href="https://github.com/microsoft/autogen/">AutoGen repository</a> or contact me directly at <a href="mailto:beibin.li@microsoft.com">beibin.li@microsoft.com</a>.</p><p>AutoGen will continue to evolve, incorporating more multimodal functionalities such as DALLE model integration, audio interaction, and video comprehension. Stay tuned for these exciting developments.</p>]]></content>
        <author>
            <name>Beibin Li</name>
            <uri>https://github.com/beibinli</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoGen's TeachableAgent]]></title>
        <id>AutoGen's TeachableAgent</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/10/26/TeachableAgent"/>
        <updated>2023-10-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Teachable Agent Architecture]]></summary>
        <content type="html"><![CDATA[<p><img src="img/teachable-arch.png" alt="Teachable Agent Architecture"/></p><p><strong>TL;DR:</strong></p><ul><li>We introduce <strong>TeachableAgent</strong> (which uses <strong>TextAnalyzerAgent</strong>) so that users can teach their LLM-based assistants new facts, preferences, and skills.</li><li>We showcase examples of <code>TeachableAgent</code> learning and later recalling facts, preferences, and skills in subsequent chats.</li></ul><h2>Introduction</h2><p>Conversational assistants based on LLMs can remember the current chat with the user, and can also demonstrate in-context learning of user teachings during the conversation. But the assistant&#x27;s memories and learnings are lost once the chat is over, or when a single chat grows too long for the LLM to handle effectively. Then in subsequent chats the user is forced to repeat any necessary instructions over and over.</p><p><code>TeachableAgent</code> addresses these limitations by persisting user teachings across chat boundaries in long-term memory implemented as a vector database. Memory is automatically saved to disk at the end of each chat, then loaded from disk at the start of the next. Instead of copying all of memory into the context window, which would eat up valuable space, individual memories (called memos) are retrieved into context as needed. This allows the user to teach frequently used facts and skills to the teachable agent just once, and have it recall them in later chats.</p><p>In order to make effective decisions about memo storage and retrieval, <code>TeachableAgent</code> calls an instance of <code>TextAnalyzerAgent</code> (another AutoGen agent) to identify and reformulate text as needed for remembering facts, preferences, and skills. Note that this adds extra LLM calls involving a relatively small number of tokens, which can add a few seconds to the time a user waits for each response.</p><h2>Run It Yourself</h2><p>AutoGen contains three code examples that use <code>TeachableAgent</code>.</p><ol><li><p>Run <a href="https://github.com/microsoft/autogen/blob/main/test/agentchat/contrib/chat_with_teachable_agent.py">chat_with_teachable_agent.py</a> to converse with <code>TeachableAgent</code>.</p></li><li><p>Use the Jupyter notebook <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_teachability.ipynb">agentchat_teachability.ipynb</a> to step through examples discussed below.</p></li><li><p>Run <a href="https://github.com/microsoft/autogen/blob/main/test/agentchat/contrib/test_teachable_agent.py">test_teachable_agent.py</a> for quick unit testing of <code>TeachableAgent</code>.</p></li></ol><h2>Basic Usage of TeachableAgent</h2><ol><li>Install dependencies</li></ol><p>Please install pyautogen with the <!-- -->[teachable]<!-- --> option before using TeachableAgent.</p><pre><code class="language-bash">pip install &quot;pyautogen[teachable]&quot;
</code></pre><ol start="2"><li>Import agents</li></ol><pre><code class="language-python">from autogen import UserProxyAgent, config_list_from_json
from autogen.agentchat.contrib.teachable_agent import TeachableAgent
</code></pre><ol start="3"><li>Create llm_config</li></ol><pre><code class="language-python"># Load LLM inference endpoints from an env variable or a file
# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints
# and OAI_CONFIG_LIST_sample
filter_dict = {&quot;model&quot;: [&quot;gpt-4&quot;]}  # GPT-3.5 is less reliable than GPT-4 at learning from user feedback.
config_list = config_list_from_json(env_or_file=&quot;OAI_CONFIG_LIST&quot;, filter_dict=filter_dict)
llm_config={&quot;config_list&quot;: config_list, &quot;timeout&quot;: 120}
</code></pre><ol start="4"><li>Create the agents</li></ol><pre><code class="language-python">teachable_agent = TeachableAgent(
    name=&quot;teachableagent&quot;,
    llm_config=llm_config,
    teach_config={
        &quot;reset_db&quot;: False,  # Use True to force-reset the memo DB, and False to use an existing DB.
        &quot;path_to_db_dir&quot;: &quot;./tmp/interactive/teachable_agent_db&quot;  # Can be any path.
    }
)

user = UserProxyAgent(&quot;user&quot;, human_input_mode=&quot;ALWAYS&quot;)
</code></pre><ol start="5"><li>Chat with <code>TeachableAgent</code></li></ol><pre><code class="language-python"># This function will return once the user types &#x27;exit&#x27;.
teachable_agent.initiate_chat(user, message=&quot;Hi, I&#x27;m a teachable user assistant! What&#x27;s on your mind?&quot;)
</code></pre><ol start="6"><li>Update the database on disk</li></ol><pre><code class="language-python"># Before closing the app, let the teachable agent store things that should be learned from this chat.
teachable_agent.learn_from_user_feedback()
teachable_agent.close_db()
</code></pre><h2>Example 1 - Learning user info</h2><p>A user can teach the agent facts about themselves.
(Note that due to their finetuning, LLMs can be reluctant to admit that they know personal information.)</p><pre><code>CLEARING MEMORY
teachableagent (to user):

Hi, I&#x27;m a teachable user assistant! What&#x27;s on your mind?

--------------------------------------------------------------------------------
Provide feedback to teachableagent. Press enter to skip and use auto-reply, or type &#x27;exit&#x27; to end the conversation: My name is Ricky
user (to teachableagent):

My name is Ricky

--------------------------------------------------------------------------------
teachableagent (to user):

Hello, Ricky! It&#x27;s nice to meet you. What can I help you with today?

--------------------------------------------------------------------------------
Provide feedback to teachableagent. Press enter to skip and use auto-reply, or type &#x27;exit&#x27; to end the conversation: exit

REVIEWING CHAT FOR USER TEACHINGS TO REMEMBER

SAVING MEMORY TO DISK
    Location = ./tmp/interactive/teachable_agent_db\uid_text_dict.pkl
</code></pre><p>In a later conversation, the user can check whether the teachable agent remembers their name. (For readability, the user prompts and some logged notices are not repeated below.)</p><pre><code>--------------------------------------------------------------------------------
user (to teachableagent):

what&#x27;s my name?

--------------------------------------------------------------------------------
teachableagent (to user):

Hi Ricky! How can I help you today?

--------------------------------------------------------------------------------
</code></pre><h2>Example 2 - Learning new facts</h2><p>A user can teach the agent more complex, related facts.</p><pre><code>--------------------------------------------------------------------------------
user (to teachableagent):

What is the Vicuna model?

--------------------------------------------------------------------------------
teachableagent (to user):

I&#x27;m sorry, but I don&#x27;t have information about the &quot;Vicuna model&quot; in my current database. Could you please provide some context, description, or details about this model so I can better assist you?

--------------------------------------------------------------------------------
user (to teachableagent):

Vicuna is a 13B-parameter language model released by Meta.

--------------------------------------------------------------------------------
teachableagent (to user):

Thank you for providing the information. The Vicuna model is a large-scale language model developed by Meta. It consists of 13 billion parameters, which is what makes it able to understand and generate human-like text across various topics. Parameters in a neural network are the weights and biases assigned to each connection between the neurons, and having more parameters enables the model to learn and capture more complex patterns and relationships in data. As an AI language model, Vicuna can be utilized in various language-related tasks, such as text completion, translation, summarization, and more.

--------------------------------------------------------------------------------
user (to teachableagent):

What is the Orca model?

--------------------------------------------------------------------------------
teachableagent (to user):

The Orca model is a yet-to-be-released language model by Meta, referring to the Research Preview API release of Meta&#x27;s advanced language model. While I don&#x27;t have specific details about the Orca model at this time, it is expected that the model would have an even larger number of parameters to further enhance its capabilities compared to its predecessor, Vicuna.

As AI language models evolve, they generally become more efficient at understanding and generating human-like text. This makes them more effective in completing a wide range of tasks, including text completion, translation, summarization, and Q&amp;A, as well as providing more accurate and coherent results in various applications.

--------------------------------------------------------------------------------
user (to teachableagent):

Orca is a 13B-parameter language model released by Microsoft. It outperforms Vicuna on most tasks.

--------------------------------------------------------------------------------
teachableagent (to user):

Thank you for the clarification. Orca is a 13-billion-parameter language model developed by Microsoft. It is designed to outperform Vicuna, the language model released by Meta. With a large number of parameters, Orca is capable of understanding and generating human-like text across diverse subjects, making it highly useful in various applications.

Orca&#x27;s advanced capabilities allow it to excel in tasks such as text completion, translation, summarization, Q&amp;A, and more. Its performance improvement over Vicuna highlights the rapid advancements in natural language processing (NLP) research and the development of increasingly capable AI language models.
</code></pre><p>Then in a later chat the teachable agent can answer questions about the facts it has been taught.
(Remember to first close the previous chat by typing &#x27;exit&#x27;.)</p><pre><code>--------------------------------------------------------------------------------
user (to teachableagent):

How does the Vicuna model compare to the Orca model?

--------------------------------------------------------------------------------
teachableagent (to user):

The Vicuna and Orca models are both 13B-parameter language models released by Meta and Microsoft, respectively. In terms of performance, Orca has been reported to outperform Vicuna on most tasks. However, without detailed information about specific tasks and benchmarks, it is difficult to provide a more comprehensive comparison. Generally speaking, both models are advanced language models that aim to provide high-quality natural language understanding and generation, but Orca appears to have an edge in terms of overall performance.

--------------------------------------------------------------------------------
</code></pre><h2>Example 3 - Learning user preferences</h2><p>A user can teach the agent how they prefer to have things done.</p><p>Be aware that a message like the next one cannot be entered as a single message through a command line because it contains a newline character.
Such messages can be entered in a Jupyter notebook, or through some UI layer like that of ChatGPT.</p><pre><code>--------------------------------------------------------------------------------
user (to teachableagent):

Please summarize this abstract.

AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang
AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

--------------------------------------------------------------------------------
teachableagent (to user):

AutoGen is an open-source framework designed to enable developers to create LLM applications with multiple customizable agents that can converse with each other to complete tasks. These agents can operate using different combinations of LLMs, human inputs, and tools, allowing developers to define agent interaction behaviors flexibly. AutoGen supports programming flexible conversation patterns using both natural language and code, making it suitable for building diverse applications with varying complexities and LLM capacities. Its effectiveness has been demonstrated through empirical studies across various domains including mathematics, coding, operations research, decision-making, and entertainment.

--------------------------------------------------------------------------------
Please summarize this abstract. When I&#x27;m summarizing an abstract, I try to make the summary contain just three short bullet points:  the title, the innovation, and the key empirical results.

AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang
AutoGen is an open-source framework that allows developers to build LLM applications via multiple agents that can converse with each other to accomplish tasks. AutoGen agents are customizable, conversable, and can operate in various modes that employ combinations of LLMs, human inputs, and tools. Using AutoGen, developers can also flexibly define agent interaction behaviors. Both natural language and computer code can be used to program flexible conversation patterns for different applications. AutoGen serves as a generic infrastructure to build diverse applications of various complexities and LLM capacities. Empirical studies demonstrate the effectiveness of the framework in many example applications, with domains ranging from mathematics, coding, question answering, operations research, online decision-making, entertainment, etc.

--------------------------------------------------------------------------------
teachableagent (to user):

- Title: AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation
- Innovation: Open-source framework for creating customizable LLM applications through agent conversations, supporting various modes and interaction behaviors.
- Key Empirical Results: Demonstrated effectiveness across diverse application domains, including mathematics, coding, question answering, and more.

--------------------------------------------------------------------------------
</code></pre><p>Then in later chats the teacher doesn&#x27;t need to reiterate their detailed preferences.</p><pre><code>--------------------------------------------------------------------------------
user (to teachableagent):

Please summarize this abstract.

Sparks of Artificial General Intelligence: Early experiments with GPT-4
Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang
Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google&#x27;s PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4&#x27;s performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4&#x27;s capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.

--------------------------------------------------------------------------------
teachableagent (to user):

- Title: Sparks of Artificial General Intelligence: Early experiments with GPT-4
- Innovation: GPT-4, an LLM with remarkable capabilities, demonstrates human-level performance across various domains, like math, coding, vision, medicine, law, and psychology.
- Key results: GPT-4 significantly surpasses prior models, suggesting it may be an early version of AGI; limitations and challenges toward deeper AGI are also discussed.

--------------------------------------------------------------------------------
</code></pre><h2>Example 4 - Learning new skills</h2><p>Users can extend the teachable agent&#x27;s capabilities by teaching it new skills for accomplishing challenging tasks. It usually works best to first describe the task, then (in the same turn) provide a hint or advice for approaching the task.</p><p>The <a href="https://arxiv.org/abs/2303.12712">Sparks of AGI</a> paper evaluated GPT-4 on math problems like the following, which it could only solve 32% of the time. We first show a failure case, then teach the agent a strategy which lifts GPT-4&#x27;s success rate above 95%.</p><pre><code>--------------------------------------------------------------------------------
user (to teachableagent):

Consider the identity:
9 * 4 + 6 * 6 = 72
Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?
-Let&#x27;s think step-by-step, write down a plan, and then write down your solution as: &quot;The solution is: A * B + C * D&quot;.

--------------------------------------------------------------------------------
Step 1: Identify the current equation and the target value on the right-hand side.
Current equation: 9 * 4 + 6 * 6 = 72
Target value: 99

Step 2: Determine what numbers can be changed on the left-hand side.
Possible changes: 9, 4, 6 (first occurrence), and 6 (second occurrence)

Step 3: Evaluate whether each change in the numbers on the left-hand side can result in the target value.
9 -&gt; If changed, it would create a larger product.
4 -&gt; If changed, the product would increase or decrease by more than the amount needed to reach 99.
6 (first occurrence) -&gt; If increased to 7, then the equation would be 9 * 4 + 7 * 6 = 94, which is still not 99.
6 (second occurrence) -&gt; If increased to 7, then the equation would be 9 * 4 + 6 * 7 = 99.

Step 4: Write down the solution.
The solution is: 9 * 4 + 6 * 7

--------------------------------------------------------------------------------
user (to teachableagent):

Consider the identity:
9 * 4 + 6 * 6 = 72
Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 99?
-Let&#x27;s think step-by-step, write down a plan, and then write down your solution as: &quot;The solution is: A * B + C * D&quot;.

Here&#x27;s some advice that may help:
1. Let E denote the original number on the right.
2. Let F denote the final number on the right.
3. Calculate the difference between the two, G = F - E.
4. Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.
5. Calculate J = G / H. This is the number of times that H divides into G.
6. Verify that J is an integer, and that H * J = G.
7. Find the number on the left which is multiplied by H, and call it K.
8. Change K to K + J.
9. Recompute the value on the left, and verify that it equals F.
Finally, write down your solution as: &quot;The solution is: A * B + C * D&quot;.

--------------------------------------------------------------------------------
teachableagent (to user):

Step 1: Identify the original number (E) and the final number (F) on the right-hand side.
E = 72
F = 99

Step 2: Calculate the difference between the two, G = F - E.
G = 99 - 72 = 27

Step 3: Examine the numbers on the left one by one until finding one that divides evenly into G, where negative integers are allowed.
9 (divides evenly into 27)
4 (does not)
6 (first occurrence, does not)
6 (second occurrence, does not)

Step 4: Calculate J = G / H. This is the number of times that H divides into G.
H = 9 (since 9 divides evenly into 27)
J = 27 / 9 = 3

Step 5: Verify that J is an integer, and that H * J = G.
H * J = 9 * 3 = 27
J = 3 (which is an integer)

Step 6: Find the number on the left which is multiplied by H, and call it K.
Number on the leftside of the equation that is multiplied by 9: 4
Therefore, K = 4

Step 7: Change K to K + J.
K + J = 4 + 3 = 7

Step 8: Recompute the value on the left, and verify that it equals F.
9 * 7 + 6 * 6 = 63 + 36 = 99 (which is equal to F)

Step 9: Write down the solution.
The solution is: 9 * 7 + 6 * 6

--------------------------------------------------------------------------------
</code></pre><p>In a later chat the user doesn&#x27;t need to repeat the detailed advice.</p><pre><code>--------------------------------------------------------------------------------
user (to teachableagent):

Consider the identity:
8 * 3 + 7 * 9 = 87
Can you modify exactly one integer (and not more than that!) on the left hand side of the equation so the right hand side becomes 59?
-Let&#x27;s think step-by-step, write down a plan, and then write down your solution as: &quot;The solution is: A * B + C * D&quot;.

--------------------------------------------------------------------------------
teachableagent (to user):

Let&#x27;s follow the advice to modify one integer on the left hand side of the equation to make the right hand side become 59:

1. Let E denote the original number on the right: E = 87.
2. Let F denote the final number on the right: F = 59.
3. Calculate the difference between the two, G = F - E: G = 59 - 87 = -28.
4. Examine the numbers on the left one by one to find one that divides evenly into G: 8, 3, 7, 9. Since 7 divides -28 evenly, we will work with 7 (H = 7).
5. Calculate J = G / H: J = -28 / 7 = -4.
6. Verify that J is an integer, and that H * J = G: J is an integer, and 7 * (-4) = -28.
7. Find the number on the left which is multiplied by H (7), and call it K: K = 9.
8. Change K to K + J: K_new = 9 + (-4) = 5.
9. Recompute the value on the left, and verify that it equals F: (8 * 3) + (7 * 5) = 24 + 35 = 59.

The solution is: 8 * 3 + 7 * 5

--------------------------------------------------------------------------------
</code></pre><h2>Planned improvements</h2><ul><li>Instructions for making any AutoGen agent user-teachable.</li><li>Examples of how to include <code>TeachableAgent</code> in group chats.</li><li>Expansions of AutoGen&#x27;s current coding-testing strengths.</li><li>Teachability enhancements:<ul><li>Understanding user instructions distributed over multiple turns.</li><li>Learning from the agent&#x27;s own experience, to reduce dependence on explicit user instructions.</li><li>Learning skills built on top of previously learned skills.</li></ul></li></ul><h2>Conclusion</h2><p><code>TeachableAgent</code> is still under active research and development. For any problems you find or improvements you have in mind, please join our discussions in this repo and on our <a href="https://discord.gg/pAbnFJrkgZ">Discord channel</a>. We look forward to seeing how you and the rest of the community can use and improve <code>TeachableAgent</code> and the other agents in AutoGen!</p>]]></content>
        <author>
            <name>Ricky Loynd</name>
            <uri>https://github.com/rickyloynd-microsoft</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retrieval-Augmented Generation (RAG) Applications with AutoGen]]></title>
        <id>Retrieval-Augmented Generation (RAG) Applications with AutoGen</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/10/18/RetrieveChat"/>
        <updated>2023-10-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[RAG Architecture]]></summary>
        <content type="html"><![CDATA[<p><img src="img/retrievechat-arch.png" alt="RAG Architecture"/></p><p><strong>TL;DR:</strong></p><ul><li>We introduce <strong>RetrieveUserProxyAgent</strong> and <strong>RetrieveAssistantAgent</strong>, RAG agents of AutoGen that
allows retrieval-augmented generation, and its basic usage.</li><li>We showcase customizations of RAG agents, such as customizing the embedding function, the text
split function and vector database.</li><li>We also showcase two advanced usage of RAG agents, integrating with group chat and building a Chat
application with Gradio.</li></ul><h2>Introduction</h2><p>Retrieval augmentation has emerged as a practical and effective approach for mitigating the intrinsic
limitations of LLMs by incorporating external documents. In this blog post, we introduce RAG agents of
AutoGen that allows retrieval-augmented generation. The system consists of two agents: a
Retrieval-augmented User Proxy agent, called <code>RetrieveUserProxyAgent</code>, and a Retrieval-augmented Assistant
agent, called <code>RetrieveAssistantAgent</code>, both of which are extended from built-in agents from AutoGen.
The overall architecture of the RAG agents is shown in the figure above.</p><p>To use Retrieval-augmented Chat, one needs to initialize two agents including Retrieval-augmented
User Proxy and Retrieval-augmented Assistant. Initializing the Retrieval-Augmented User Proxy
necessitates specifying a path to the document collection. Subsequently, the Retrieval-Augmented
User Proxy can download the documents, segment them into chunks of a specific size, compute
embeddings, and store them in a vector database. Once a chat is initiated, the agents collaboratively
engage in code generation or question-answering adhering to the procedures outlined below:</p><ol><li>The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding similarity,
and sends them along with the question to the Retrieval-Augmented Assistant.</li><li>The Retrieval-Augmented Assistant employs an LLM to generate code or text as answers based
on the question and context provided. If the LLM is unable to produce a satisfactory response, it
is instructed to reply with “Update Context” to the Retrieval-Augmented User Proxy.</li><li>If a response includes code blocks, the Retrieval-Augmented User Proxy executes the code and
sends the output as feedback. If there are no code blocks or instructions to update the context, it
terminates the conversation. Otherwise, it updates the context and forwards the question along
with the new context to the Retrieval-Augmented Assistant. Note that if human input solicitation
is enabled, individuals can proactively send any feedback, including Update Context”, to the
Retrieval-Augmented Assistant.</li><li>If the Retrieval-Augmented Assistant receives “Update Context”, it requests the next most similar
chunks of documents as new context from the Retrieval-Augmented User Proxy. Otherwise, it
generates new code or text based on the feedback and chat history. If the LLM fails to generate
an answer, it replies with “Update Context” again. This process can be repeated several times.
The conversation terminates if no more documents are available for the context.</li></ol><h2>Basic Usage of RAG Agents</h2><ol start="0"><li>Install dependencies</li></ol><p>Please install pyautogen with the <!-- -->[retrievechat]<!-- --> option before using RAG agents.</p><pre><code class="language-bash">pip install &quot;pyautogen[retrievechat]&quot;
</code></pre><p>RetrieveChat can handle various types of documents. By default, it can process
plain text and PDF files, including formats such as &#x27;txt&#x27;, &#x27;json&#x27;, &#x27;csv&#x27;, &#x27;tsv&#x27;,
&#x27;md&#x27;, &#x27;html&#x27;, &#x27;htm&#x27;, &#x27;rtf&#x27;, &#x27;rst&#x27;, &#x27;jsonl&#x27;, &#x27;log&#x27;, &#x27;xml&#x27;, &#x27;yaml&#x27;, &#x27;yml&#x27; and &#x27;pdf&#x27;.
If you install <a href="https://unstructured-io.github.io/unstructured/installation/full_installation.html">unstructured</a>
(<code>pip install &quot;unstructured[all-docs]&quot;</code>), additional document types such as &#x27;docx&#x27;,
&#x27;doc&#x27;, &#x27;odt&#x27;, &#x27;pptx&#x27;, &#x27;ppt&#x27;, &#x27;xlsx&#x27;, &#x27;eml&#x27;, &#x27;msg&#x27;, &#x27;epub&#x27; will also be supported.</p><p>You can find a list of all supported document types by using <code>autogen.retrieve_utils.TEXT_FORMATS</code>.</p><ol><li>Import Agents</li></ol><pre><code class="language-python">import autogen
from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent
from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent
</code></pre><ol start="2"><li>Create an &#x27;RetrieveAssistantAgent&#x27; instance named &quot;assistant&quot; and an &#x27;RetrieveUserProxyAgent&#x27; instance named &quot;ragproxyagent&quot;</li></ol><pre><code class="language-python">assistant = RetrieveAssistantAgent(
    name=&quot;assistant&quot;,
    system_message=&quot;You are a helpful assistant.&quot;,
    llm_config=llm_config,
)

ragproxyagent = RetrieveUserProxyAgent(
    name=&quot;ragproxyagent&quot;,
    retrieve_config={
        &quot;task&quot;: &quot;qa&quot;,
        &quot;docs_path&quot;: &quot;https://raw.githubusercontent.com/microsoft/autogen/main/README.md&quot;,
    },
)
</code></pre><ol start="3"><li>Initialize Chat and ask a question</li></ol><pre><code class="language-python">assistant.reset()
ragproxyagent.initiate_chat(assistant, problem=&quot;What is autogen?&quot;)
</code></pre><p>Output is like:</p><pre><code>--------------------------------------------------------------------------------
assistant (to ragproxyagent):

AutoGen is a framework that enables the development of large language model (LLM) applications using multiple agents that can converse with each other to solve tasks. The agents are customizable, conversable, and allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.

--------------------------------------------------------------------------------
</code></pre><ol start="4"><li>Create a UserProxyAgent and ask the same question</li></ol><pre><code class="language-python">assistant.reset()
userproxyagent = autogen.UserProxyAgent(name=&quot;userproxyagent&quot;)
userproxyagent.initiate_chat(assistant, message=&quot;What is autogen?&quot;)
</code></pre><p>Output is like:</p><pre><code>--------------------------------------------------------------------------------
assistant (to userproxyagent):

In computer software, autogen is a tool that generates program code automatically, without the need for manual coding. It is commonly used in fields such as software engineering, game development, and web development to speed up the development process and reduce errors. Autogen tools typically use pre-programmed rules, templates, and data to create code for repetitive tasks, such as generating user interfaces, database schemas, and data models. Some popular autogen tools include Visual Studio&#x27;s Code Generator and Unity&#x27;s Asset Store.

--------------------------------------------------------------------------------
</code></pre><p>You can see that the output of <code>UserProxyAgent</code> is not related to our <code>autogen</code> since the latest info of
<code>autogen</code> is not in ChatGPT&#x27;s training data. The output of <code>RetrieveUserProxyAgent</code> is correct as it can
perform retrieval-augmented generation based on the given documentation file.</p><h2>Customizing RAG Agents</h2><p><code>RetrieveUserProxyAgent</code> is customizable with <code>retrieve_config</code>. There are several parameters to configure
based on different use cases. In this section, we&#x27;ll show how to customize embedding function, text split
function and vector database.</p><h3>Customizing Embedding Function</h3><p>By default, <a href="https://www.sbert.net">Sentence Transformers</a> and its pretrained models will be used to
compute embeddings. It&#x27;s possible that you want to use OpenAI, Cohere, HuggingFace or other embedding functions.</p><ul><li>OpenAI</li></ul><pre><code class="language-python">from chromadb.utils import embedding_functions

openai_ef = embedding_functions.OpenAIEmbeddingFunction(
                api_key=&quot;YOUR_API_KEY&quot;,
                model_name=&quot;text-embedding-ada-002&quot;
            )

ragproxyagent = RetrieveUserProxyAgent(
    name=&quot;ragproxyagent&quot;,
    retrieve_config={
        &quot;task&quot;: &quot;qa&quot;,
        &quot;docs_path&quot;: &quot;https://raw.githubusercontent.com/microsoft/autogen/main/README.md&quot;,
        &quot;embedding_function&quot;: openai_ef,
    },
)
</code></pre><ul><li>HuggingFace</li></ul><pre><code class="language-python">huggingface_ef = embedding_functions.HuggingFaceEmbeddingFunction(
    api_key=&quot;YOUR_API_KEY&quot;,
    model_name=&quot;sentence-transformers/all-MiniLM-L6-v2&quot;
)
</code></pre><p>More examples can be found <a href="https://docs.trychroma.com/embeddings">here</a>.</p><h3>Customizing Text Split Function</h3><p>Before we can store the documents into a vector database, we need to split the texts into chunks. Although
we have implemented a flexible text splitter in autogen, you may still want to use different text splitters.
There are also some existing text split tools which are good to reuse.</p><p>For example, you can use all the text splitters in langchain.</p><pre><code class="language-python">from langchain.text_splitter import RecursiveCharacterTextSplitter

recur_spliter = RecursiveCharacterTextSplitter(separators=[&quot;\n&quot;, &quot;\r&quot;, &quot;\t&quot;])

ragproxyagent = RetrieveUserProxyAgent(
    name=&quot;ragproxyagent&quot;,
    retrieve_config={
        &quot;task&quot;: &quot;qa&quot;,
        &quot;docs_path&quot;: &quot;https://raw.githubusercontent.com/microsoft/autogen/main/README.md&quot;,
        &quot;custom_text_split_function&quot;: recur_spliter.split_text,
    },
)
</code></pre><h3>Customizing Vector Database</h3><p>We are using chromadb as the default vector database, you can also replace it with any other vector database
by simply overriding the function <code>retrieve_docs</code> of <code>RetrieveUserProxyAgent</code>.</p><p>For example, you can use Qdrant as below:</p><pre><code class="language-python"># Creating qdrant client
from qdrant_client import QdrantClient

client = QdrantClient(url=&quot;***&quot;, api_key=&quot;***&quot;)

# Wrapping RetrieveUserProxyAgent
from litellm import embedding as test_embedding
from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent
from qdrant_client.models import SearchRequest, Filter, FieldCondition, MatchText

class QdrantRetrieveUserProxyAgent(RetrieveUserProxyAgent):
    def query_vector_db(
        self,
        query_texts: List[str],
        n_results: int = 10,
        search_string: str = &quot;&quot;,
        **kwargs,
    ) -&gt; Dict[str, Union[List[str], List[List[str]]]]:
        # define your own query function here
        embed_response = test_embedding(&#x27;text-embedding-ada-002&#x27;, input=query_texts)

        all_embeddings: List[List[float]] = []

        for item in embed_response[&#x27;data&#x27;]:
            all_embeddings.append(item[&#x27;embedding&#x27;])

        search_queries: List[SearchRequest] = []

        for embedding in all_embeddings:
            search_queries.append(
                SearchRequest(
                    vector=embedding,
                    filter=Filter(
                        must=[
                            FieldCondition(
                                key=&quot;page_content&quot;,
                                match=MatchText(
                                    text=search_string,
                                )
                            )
                        ]
                    ),
                    limit=n_results,
                    with_payload=True,
                )
            )

        search_response = client.search_batch(
            collection_name=&quot;{your collection name}&quot;,
            requests=search_queries,
        )

        return {
            &quot;ids&quot;: [[scored_point.id for scored_point in batch] for batch in search_response],
            &quot;documents&quot;: [[scored_point.payload.get(&#x27;page_content&#x27;, &#x27;&#x27;) for scored_point in batch] for batch in search_response],
            &quot;metadatas&quot;: [[scored_point.payload.get(&#x27;metadata&#x27;, {}) for scored_point in batch] for batch in search_response]
        }

    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = &quot;&quot;, **kwargs):
        results = self.query_vector_db(
            query_texts=[problem],
            n_results=n_results,
            search_string=search_string,
            **kwargs,
        )

        self._results = results


# Use QdrantRetrieveUserProxyAgent
qdrantragagent = QdrantRetrieveUserProxyAgent(
    name=&quot;ragproxyagent&quot;,
    human_input_mode=&quot;NEVER&quot;,
    max_consecutive_auto_reply=2,
    retrieve_config={
        &quot;task&quot;: &quot;qa&quot;,
    },
)

qdrantragagent.retrieve_docs(&quot;What is Autogen?&quot;, n_results=10, search_string=&quot;autogen&quot;)
</code></pre><h2>Advanced Usage of RAG Agents</h2><h3>Integrate with other agents in a group chat</h3><p>To use <code>RetrieveUserProxyAgent</code> in a group chat is almost the same as you use it in a two agents chat. The only thing is that
you need to <strong>initialize the chat with <code>RetrieveUserProxyAgent</code></strong>. The <code>RetrieveAssistantAgent</code> is not necessary in a group chat.</p><p>However, you may want to initialize the chat with another agent in some cases. To leverage the best of <code>RetrieveUserProxyAgent</code>,
you&#x27;ll need to call it from a function.</p><pre><code class="language-python">llm_config = {
    &quot;functions&quot;: [
        {
            &quot;name&quot;: &quot;retrieve_content&quot;,
            &quot;description&quot;: &quot;retrieve content for code generation and question answering.&quot;,
            &quot;parameters&quot;: {
                &quot;type&quot;: &quot;object&quot;,
                &quot;properties&quot;: {
                    &quot;message&quot;: {
                        &quot;type&quot;: &quot;string&quot;,
                        &quot;description&quot;: &quot;Refined message which keeps the original meaning and can be used to retrieve content for code generation and question answering.&quot;,
                    }
                },
                &quot;required&quot;: [&quot;message&quot;],
            },
        },
    ],
    &quot;config_list&quot;: config_list,
    &quot;timeout&quot;: 60,
    &quot;seed&quot;: 42,
}

boss = autogen.UserProxyAgent(
    name=&quot;Boss&quot;,
    is_termination_msg=termination_msg,
    human_input_mode=&quot;TERMINATE&quot;,
    system_message=&quot;The boss who ask questions and give tasks.&quot;,
)

boss_aid = RetrieveUserProxyAgent(
    name=&quot;Boss_Assistant&quot;,
    is_termination_msg=termination_msg,
    system_message=&quot;Assistant who has extra content retrieval power for solving difficult problems.&quot;,
    human_input_mode=&quot;NEVER&quot;,
    max_consecutive_auto_reply=3,
    retrieve_config={
        &quot;task&quot;: &quot;qa&quot;,
    },
    code_execution_config=False,  # we don&#x27;t want to execute code in this case.
)

coder = AssistantAgent(
    name=&quot;Senior_Python_Engineer&quot;,
    is_termination_msg=termination_msg,
    system_message=&quot;You are a senior python engineer. Reply `TERMINATE` in the end when everything is done.&quot;,
    llm_config=llm_config,
)

pm = autogen.AssistantAgent(
    name=&quot;Product_Manager&quot;,
    is_termination_msg=termination_msg,
    system_message=&quot;You are a product manager. Reply `TERMINATE` in the end when everything is done.&quot;,
    llm_config=llm_config,
)

reviewer = autogen.AssistantAgent(
    name=&quot;Code_Reviewer&quot;,
    is_termination_msg=termination_msg,
    system_message=&quot;You are a code reviewer. Reply `TERMINATE` in the end when everything is done.&quot;,
    llm_config=llm_config,
)

def retrieve_content(message, n_results=3):
        boss_aid.n_results = n_results  # Set the number of results to be retrieved.
        # Check if we need to update the context.
        update_context_case1, update_context_case2 = boss_aid._check_update_context(message)
        if (update_context_case1 or update_context_case2) and boss_aid.update_context:
            boss_aid.problem = message if not hasattr(boss_aid, &quot;problem&quot;) else boss_aid.problem
            _, ret_msg = boss_aid._generate_retrieve_user_reply(message)
        else:
            ret_msg = boss_aid.generate_init_message(message, n_results=n_results)
        return ret_msg if ret_msg else message

for agent in [boss, coder, pm, reviewer]:
    # register functions for all agents.
    agent.register_function(
        function_map={
            &quot;retrieve_content&quot;: retrieve_content,
        }
    )

groupchat = autogen.GroupChat(
    agents=[boss, coder, pm, reviewer], messages=[], max_round=12
)
manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)

# Start chatting with the boss as this is the user proxy agent.
boss.initiate_chat(
    manager,
    message=&quot;How to use spark for parallel training in FLAML? Give me sample code.&quot;,
)
</code></pre><h3>Build a Chat application with Gradio</h3><p>Now, let&#x27;s wrap it up and make a Chat application with AutoGen and Gradio.</p><p><img src="img/autogen-rag.gif" alt="RAG ChatBot with AutoGen"/></p><pre><code class="language-python"># Initialize Agents
def initialize_agents(config_list, docs_path=None):
    ...
    return assistant, ragproxyagent

# Initialize Chat
def initiate_chat(config_list, problem, queue, n_results=3):
    ...
    assistant.reset()
    try:
        ragproxyagent.a_initiate_chat(
            assistant, problem=problem, silent=False, n_results=n_results
        )
        messages = ragproxyagent.chat_messages
        messages = [messages[k] for k in messages.keys()][0]
        messages = [m[&quot;content&quot;] for m in messages if m[&quot;role&quot;] == &quot;user&quot;]
        print(&quot;messages: &quot;, messages)
    except Exception as e:
        messages = [str(e)]
    queue.put(messages)

# Wrap AutoGen part into a function
def chatbot_reply(input_text):
    &quot;&quot;&quot;Chat with the agent through terminal.&quot;&quot;&quot;
    queue = mp.Queue()
    process = mp.Process(
        target=initiate_chat,
        args=(config_list, input_text, queue),
    )
    process.start()
    try:
        messages = queue.get(timeout=TIMEOUT)
    except Exception as e:
        messages = [str(e) if len(str(e)) &gt; 0 else &quot;Invalid Request to OpenAI, please check your API keys.&quot;]
    finally:
        try:
            process.terminate()
        except:
            pass
    return messages

...

# Set up UI with Gradio
with gr.Blocks() as demo:
    ...
    assistant, ragproxyagent = initialize_agents(config_list)

    chatbot = gr.Chatbot(
        [],
        elem_id=&quot;chatbot&quot;,
        bubble_full_width=False,
        avatar_images=(None, (os.path.join(os.path.dirname(__file__), &quot;autogen.png&quot;))),
        # height=600,
    )

    txt_input = gr.Textbox(
        scale=4,
        show_label=False,
        placeholder=&quot;Enter text and press enter&quot;,
        container=False,
    )

    with gr.Row():
        txt_model = gr.Dropdown(
            label=&quot;Model&quot;,
            choices=[
                &quot;gpt-4&quot;,
                &quot;gpt-35-turbo&quot;,
                &quot;gpt-3.5-turbo&quot;,
            ],
            allow_custom_value=True,
            value=&quot;gpt-35-turbo&quot;,
            container=True,
        )
        txt_oai_key = gr.Textbox(
            label=&quot;OpenAI API Key&quot;,
            placeholder=&quot;Enter key and press enter&quot;,
            max_lines=1,
            show_label=True,
            value=os.environ.get(&quot;OPENAI_API_KEY&quot;, &quot;&quot;),
            container=True,
            type=&quot;password&quot;,
        )
        ...

    clear = gr.ClearButton([txt_input, chatbot])

...

if __name__ == &quot;__main__&quot;:
    demo.launch(share=True)
</code></pre><p>The online app and the source code are hosted in <a href="https://huggingface.co/spaces/thinkall/autogen-demos">HuggingFace</a>. Feel free to give it a try!</p><h2>Read More</h2><p>You can check out more example notebooks for RAG use cases:</p><ul><li><a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_RetrieveChat.ipynb">Automated Code Generation and Question Answering with Retrieval Augmented Agents</a></li><li><a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_RAG.ipynb">Group Chat with Retrieval Augmented Generation (with 5 group member agents and 1 manager agent)</a></li><li><a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_qdrant_RetrieveChat.ipynb">Automated Code Generation and Question Answering with Qdrant based Retrieval Augmented Agents</a></li></ul>]]></content>
        <author>
            <name>Li Jiang</name>
            <uri>https://github.com/thinkall</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use AutoGen for Local LLMs]]></title>
        <id>Use AutoGen for Local LLMs</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/07/14/Local-LLMs"/>
        <updated>2023-07-14T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[TL;DR:]]></summary>
        <content type="html"><![CDATA[<p><strong>TL;DR:</strong>
We demonstrate how to use autogen for local LLM application. As an example, we will initiate an endpoint using <a href="https://github.com/lm-sys/FastChat">FastChat</a> and perform inference on <a href="https://github.com/THUDM/ChatGLM2-6B">ChatGLMv2-6b</a>.</p><h2>Preparations</h2><h3>Clone FastChat</h3><p>FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. However, its code needs minor modification in order to function properly.</p><pre><code class="language-bash">git clone https://github.com/lm-sys/FastChat.git
cd FastChat
</code></pre><h3>Download checkpoint</h3><p>ChatGLM-6B is an open bilingual language model based on General Language Model (GLM) framework, with 6.2 billion parameters. ChatGLM2-6B is its second-generation version.</p><p>Before downloading from HuggingFace Hub, you need to have Git LFS <a href="https://docs.github.com/en/repositories/working-with-files/managing-large-files/installing-git-large-file-storage">installed</a>.</p><pre><code class="language-bash">git clone https://huggingface.co/THUDM/chatglm2-6b
</code></pre><h2>Initiate server</h2><p>First, launch the controller</p><pre><code class="language-bash">python -m fastchat.serve.controller
</code></pre><p>Then, launch the model worker(s)</p><pre><code class="language-bash">python -m fastchat.serve.model_worker --model-path chatglm2-6b
</code></pre><p>Finally, launch the RESTful API server</p><pre><code class="language-bash">python -m fastchat.serve.openai_api_server --host localhost --port 8000
</code></pre><p>Normally this will work. However, if you encounter error like <a href="https://github.com/lm-sys/FastChat/issues/1641">this</a>, commenting out all the lines containing <code>finish_reason</code> in <code>fastchat/protocol/api_protocol.py</code> and <code>fastchat/protocol/openai_api_protocol.py</code> will fix the problem. The modified code looks like:</p><pre><code class="language-python">class CompletionResponseChoice(BaseModel):
    index: int
    text: str
    logprobs: Optional[int] = None
    # finish_reason: Optional[Literal[&quot;stop&quot;, &quot;length&quot;]]

class CompletionResponseStreamChoice(BaseModel):
    index: int
    text: str
    logprobs: Optional[float] = None
    # finish_reason: Optional[Literal[&quot;stop&quot;, &quot;length&quot;]] = None
</code></pre><h2>Interact with model using <code>oai.Completion</code> (requires openai&lt;1)</h2><p>Now the models can be directly accessed through openai-python library as well as <code>autogen.oai.Completion</code> and <code>autogen.oai.ChatCompletion</code>.</p><pre><code class="language-python">from autogen import oai

# create a text completion request
response = oai.Completion.create(
    config_list=[
        {
            &quot;model&quot;: &quot;chatglm2-6b&quot;,
            &quot;base_url&quot;: &quot;http://localhost:8000/v1&quot;,
            &quot;api_type&quot;: &quot;open_ai&quot;,
            &quot;api_key&quot;: &quot;NULL&quot;, # just a placeholder
        }
    ],
    prompt=&quot;Hi&quot;,
)
print(response)

# create a chat completion request
response = oai.ChatCompletion.create(
    config_list=[
        {
            &quot;model&quot;: &quot;chatglm2-6b&quot;,
            &quot;base_url&quot;: &quot;http://localhost:8000/v1&quot;,
            &quot;api_type&quot;: &quot;open_ai&quot;,
            &quot;api_key&quot;: &quot;NULL&quot;,
        }
    ],
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi&quot;}]
)
print(response)
</code></pre><p>If you would like to switch to different models, download their checkpoints and specify model path when launching model worker(s).</p><h2>interacting with multiple local LLMs</h2><p>If you would like to interact with multiple LLMs on your local machine, replace the <code>model_worker</code> step above with a multi model variant:</p><pre><code class="language-bash">python -m fastchat.serve.multi_model_worker \
    --model-path lmsys/vicuna-7b-v1.3 \
    --model-names vicuna-7b-v1.3 \
    --model-path chatglm2-6b \
    --model-names chatglm2-6b
</code></pre><p>The inference code would be:</p><pre><code class="language-python">from autogen import oai

# create a chat completion request
response = oai.ChatCompletion.create(
    config_list=[
        {
            &quot;model&quot;: &quot;chatglm2-6b&quot;,
            &quot;base_url&quot;: &quot;http://localhost:8000/v1&quot;,
            &quot;api_type&quot;: &quot;open_ai&quot;,
            &quot;api_key&quot;: &quot;NULL&quot;,
        },
        {
            &quot;model&quot;: &quot;vicuna-7b-v1.3&quot;,
            &quot;base_url&quot;: &quot;http://localhost:8000/v1&quot;,
            &quot;api_type&quot;: &quot;open_ai&quot;,
            &quot;api_key&quot;: &quot;NULL&quot;,
        }
    ],
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hi&quot;}]
)
print(response)
</code></pre><h2>For Further Reading</h2><ul><li><a href="/docs/Getting-Started">Documentation</a> about <code>autogen</code>.</li><li><a href="https://github.com/lm-sys/FastChat">Documentation</a> about FastChat.</li></ul>]]></content>
        <author>
            <name>Jiale Liu</name>
            <uri>https://leoljl.github.io</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MathChat - An Conversational Framework to Solve Math Problems]]></title>
        <id>MathChat - An Conversational Framework to Solve Math Problems</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/06/28/MathChat"/>
        <updated>2023-06-28T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[MathChat WorkFlow]]></summary>
        <content type="html"><![CDATA[<p><img src="img/mathchatflow.png" alt="MathChat WorkFlow"/>
<strong>TL;DR:</strong></p><ul><li><strong>We introduce MathChat, a conversational framework leveraging Large Language Models (LLMs), specifically GPT-4, to solve advanced mathematical problems.</strong></li><li><strong>MathChat improves LLM&#x27;s performance on challenging math problem-solving, outperforming basic prompting and other strategies by about 6%. The improvement was especially notable in the Algebra category, with a 15% increase in accuracy.</strong></li><li><strong>Despite the advancement, GPT-4 still struggles to solve very challenging math problems, even with effective prompting strategies. Further improvements are needed, such as the development of more specific assistant models or the integration of new tools and prompts.</strong></li></ul><p>Recent Large Language Models (LLMs) like GTP-3.5 and GPT-4 have demonstrated astonishing abilities over previous models on various tasks, such as text generation, question answering, and code generation. Moreover, these models can communicate with humans through conversations and remember previous contexts, making it easier for humans to interact with them. These models play an increasingly important role in our daily lives assisting people with different tasks, such as writing emails, summarizing documents, and writing code.</p><p>In this blog post, we probe into the problem-solving capabilities of LLMs. Specifically, we are interested in their capabilities to solve advanced math problems, which could be representative of a broader class of problems that require precise reasoning and also have deterministic solutions.</p><p>We introduce MathChat, a conversational framework designed for solving challenging math problems with LLMs. This framework takes advantage of the chat-optimized feature of state-of-the-art LLMs, where a user proxy agent and an LLM assistant work together to tackle math problems. We also test previous prompting techniques for comparison.</p><h2>The MathChat Framework</h2><p>MathChat simulates a conversation between the LLM assistant and a user proxy agent. As the name indicates, the user proxy agent acts as a proxy for the user, which is responsible for communicating with the LLM assistant and continuing the conversation in a desired manner.</p><p>The proxy agent first presents a math problem to the LLM assistant, framed by an initial prompt that guides the assistant in how it should work collaboratively to solve the problem. We incorporate existing prompting methods such as CoT (chain-of-thought) and tool-using are incorporated in the initial prompt. Key components of the prompt include:</p><ul><li><p><strong>Tool-using Prompt:</strong> This guides the LLM assistant to use Python code in the correct format to address the problem, allowing the user proxy agent to parse the code and provide the appropriate results.</p></li><li><p><strong>Problem-Solving Strategy Selection Prompt:</strong> The assistant is instructed to choose one of three potential problem-solving strategies, including:</p><ol><li>Writing a Python program to solve the problem directly.</li><li>Solving the problem directly without using Python, which exercises LLMs&#x27; inherent reasoning capabilities.</li><li>Solving the problem step by step with Python. This should be used when the first two strategies aren&#x27;t suitable.</li></ol></li><li><p><strong>Final Answer Encapsulation Prompt:</strong> This part instructs the assistant to put the final answer in <em>\boxed</em>{}.</p></li></ul><p>The prompt design in MathChat is flexible and can easily be refined to include other tools, such as Wolfram Alpha.</p><p>Let&#x27;s take a look at an example between the <strong>User Proxy Agent</strong> and the <strong>LLM Assistant</strong> (GPT-4). The conversation focuses on how to solve inequality using Python.
(The conversation is modified for readability.)</p><table><thead><tr><th>Participant</th><th>Dialogue</th></tr></thead><tbody><tr><td>User Proxy Agent</td><td>(Initial Prompt: Let&#x27;s solve a math problem with Python...) Problem: Find all x that satisfy the inequality (2x+10)(x+3) &lt; (3x+9)(x+8). Express your answer in interval notation.</td></tr><tr><td>LLM assistant</td><td>The key idea to solve this problem is to find the critical points, which are the points where the inequality is equal. Then, test the intervals between the critical points to find the interval for which the inequality holds.</td></tr><tr><td>LLM assistant (continued)</td><td>Here&#x27;s Python code to find the critical points: <code>from sympy import symbols, Eq, solve; x = symbols(&quot;x&quot;); lhs = (2*x + 10)*(x + 3); rhs = (3*x + 9)*(x + 8); equation = Eq(lhs, rhs); critical_points = solve(equation, x); critical_points</code></td></tr><tr><td>User Proxy Agent</td><td>The critical points are: <!-- -->[-14, -3]</td></tr><tr><td>LLM assistant</td><td>We found the critical points to be x = -14 and x = -3. Now we&#x27;ll test the intervals between these critical points as well as points before and after the critical points. The intervals are: x &lt; -14, -14 &lt; x &lt; -3, x &gt; -3.</td></tr><tr><td>LLM assistant (continued)</td><td>Here&#x27;s Python code to test the inequality for each interval: <code>from sympy import symbols; x = symbols(&quot;x&quot;); inequality = (2*x + 10)*(x + 3) &lt; (3*x + 9)*(x + 8); test_points = [-15, -5, 0]; intervals = []; for point in test_points: if inequality.subs(x, point): intervals.append(point); intervals</code></td></tr><tr><td>User Proxy Agent</td><td>The intervals where the inequality holds true are for test points: <!-- -->[-15, 0]</td></tr><tr><td>LLM assistant</td><td>We have tested the intervals and found that the inequality holds for: 1. x &lt; -14 (test point: -15) 2. x &gt; -3 (test point: 0). Therefore, the solution to the inequality (2x+10)(x+3) &lt; (3x+9)(x+8) is x ∈ (-∞, -14) ∪ (-3, ∞)</td></tr></tbody></table><h2>Experiment Setup</h2><p>We evaluate the improvement brought by MathChat.</p><p>For the experiment, we focus on the level-5 problems from the MATH dataset, which are composed of high school competition problems. These problems include the application of theorems and complex equation derivation and are challenging even for undergraduate students. We evaluate 6 of 7 categories from the dataset (excluding Geometry): Prealgebra, Algebra, Number Theory, Counting and Probability, Intermediate Algebra, and Precalculus.</p><p>We evaluate GPT-4 and use the default configuration of the OpenAI API. To access the final performance, we manually compare the final answer with the correct answer.  For the vanilla prompt, Program Synthesis, and MathChat, we have GPT-4 enclose the final answer in <em>\boxed</em>{}, and we take the return of the function in PoT as the final answer.</p><p>We also evaluate the following methods for comparison:</p><ol><li><p><strong>Vanilla prompting:</strong> Evaluates GPT-4&#x27;s direct problem-solving capability. The prompt used is: <em>&quot; Solve the problem carefully. Put the final answer in \boxed{}&quot;</em>.</p></li><li><p><strong>Program of Thoughts (PoT):</strong> Uses a zero-shot PoT prompt that requests the model to create a <em>Solver</em> function to solve the problem and return the final answer.</p></li><li><p><strong>Program Synthesis (PS) prompting:</strong> Like PoT, it prompts the model to write a program to solve the problem. The prompt used is: <em>&quot;Write a program that answers the following question: <!-- -->{<!-- -->Problem<!-- -->}<!-- -->&quot;</em>.</p></li></ol><h2>Experiment Results</h2><p>The accuracy on all the problems with difficulty level-5 from different categories of the MATH dataset with different methods is shown below:</p><p><img src="img/result.png" alt="Result"/></p><p>We found that compared to basic prompting, which demonstrates the innate capabilities of GPT-4, utilizing Python within the context of PoT or PS strategy improved the overall accuracy by about 10%. This increase was mostly seen in categories involving more numerical manipulations, such as Counting &amp; Probability and Number Theory, and in more complex categories like Intermediate Algebra and Precalculus.</p><p>For categories like Algebra and Prealgebra, PoT and PS showed little improvement, and in some instances, even led to a decrease in accuracy. However, MathChat was able to enhance total accuracy by around 6% compared to PoT and PS, showing competitive performance across all categories. Remarkably, MathChat improved accuracy in the Algebra category by about 15% over other methods. Note that categories like Intermediate Algebra and Precalculus remained challenging for all methods, with only about 20% of problems solved accurately.</p><p>The code for experiments can be found at this <a href="https://github.com/kevin666aa/FLAML/tree/gpt_math_solver/flaml/autogen/math">repository</a>.
We now provide an implementation of MathChat using the interactive agents in AutoGen. See this <a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_MathChat.ipynb">notebook</a> for example usage.</p><h2>Future Directions</h2><p>Despite MathChat&#x27;s improvements over previous methods, the results show that complex math problem is still challenging for recent powerful LLMs, like GPT-4, even with help from external tools.</p><p>Further work can be done to enhance this framework or math problem-solving in general:</p><ul><li>Although enabling the model to use tools like Python can reduce calculation errors, LLMs are still prone to logic errors. Methods like self-consistency (Sample several solutions and take a major vote on the final answer), or self-verification (use another LLM instance to check whether an answer is correct) might improve the performance.</li><li>Sometimes, whether the LLM can solve the problem depends on the plan it uses. Some plans require less computation and logical reasoning, leaving less room for mistakes.</li><li>MathChat has the potential to be adapted into a copilot system, which could assist users with math problems. This system could allow users to be more involved in the problem-solving process, potentially enhancing learning.</li></ul><h2>For Further Reading</h2><ul><li><a href="https://arxiv.org/abs/2306.01337">Research paper of MathChat</a></li><li><a href="/docs/Getting-Started">Documentation about <code>autogen</code></a></li></ul><p><em>Are you working on applications that involve math problem-solving? Would you appreciate additional research or support on the application of LLM-based agents for math problem-solving? Please join our <a href="https://discord.gg/pAbnFJrkgZ">Discord</a> server for discussion.</em></p>]]></content>
        <author>
            <name>Yiran Wu</name>
            <uri>https://github.com/kevin666aa</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Achieve More, Pay Less - Use GPT-4 Smartly]]></title>
        <id>Achieve More, Pay Less - Use GPT-4 Smartly</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/05/18/GPT-adaptive-humaneval"/>
        <updated>2023-05-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding success rate and inference cost]]></summary>
        <content type="html"><![CDATA[<p><img src="img/humaneval.png" alt="An adaptive way of using GPT-3.5 and GPT-4 outperforms GPT-4 in both coding success rate and inference cost"/></p><p><strong>TL;DR:</strong></p><ul><li><strong>A case study using the HumanEval benchmark shows that an adaptive way of using multiple GPT models can achieve both much higher accuracy (from 68% to 90%) and lower inference cost (by 18%) than using GPT-4 for coding.</strong></li></ul><p>GPT-4 is a big upgrade of foundation model capability, e.g., in code and math, accompanied by a much higher (more than 10x) price per token to use over GPT-3.5-Turbo. On a code completion benchmark, <a href="https://huggingface.co/datasets/openai_humaneval">HumanEval</a>, developed by OpenAI, GPT-4 can successfully solve 68% tasks while GPT-3.5-Turbo does 46%. It is possible to increase the success rate of GPT-4 further by generating multiple responses or making multiple calls. However, that will further increase the cost, which is already nearly 20 times of using GPT-3.5-Turbo and with more restricted API call rate limit. Can we achieve more with less?</p><p>In this blog post, we will explore a creative, adaptive way of using GPT models which leads to a big leap forward.</p><h2>Observations</h2><ul><li>GPT-3.5-Turbo can already solve 40%-50% tasks. For these tasks if we never use GPT-4, we can save nearly 40-50% cost.</li><li>If we use the saved cost to generate more responses with GPT-4 for the remaining unsolved tasks, it is possible to solve some more of them while keeping the amortized cost down.</li></ul><p>The obstacle of leveraging these observations is that we do not know <em>a priori</em> which tasks can be solved by the cheaper model, which tasks can be solved by the expensive model, and which tasks can be solved by paying even more to the expensive model.</p><p>To overcome that obstacle, one may want to predict which task requires what model to solve and how many responses are required for each task. Let&#x27;s look at one example code completion task:</p><pre><code class="language-python">def vowels_count(s):
    &quot;&quot;&quot;Write a function vowels_count which takes a string representing
    a word as input and returns the number of vowels in the string.
    Vowels in this case are &#x27;a&#x27;, &#x27;e&#x27;, &#x27;i&#x27;, &#x27;o&#x27;, &#x27;u&#x27;. Here, &#x27;y&#x27; is also a
    vowel, but only when it is at the end of the given word.

    Example:
    &gt;&gt;&gt; vowels_count(&quot;abcde&quot;)
    2
    &gt;&gt;&gt; vowels_count(&quot;ACEDY&quot;)
    3
    &quot;&quot;&quot;
</code></pre><p>Can we predict whether GPT-3.5-Turbo can solve this task or do we need to use GPT-4? My first guess is that GPT-3.5-Turbo can get it right because the instruction is fairly straightforward. Yet, it turns out that GPT-3.5-Turbo does not consistently get it right, if we only give it one chance. It&#x27;s not obvious (but an interesting research question!) how to predict the performance without actually trying.</p><p>What else can we do? We notice that:
<strong>It&#x27;s &quot;easier&quot; to verify a given solution than finding a correct solution from scratch.</strong></p><p>Some simple example test cases are provided in the docstr. If we already have a response generated by a model, we can use those test cases to filter wrong implementations, and either use a more powerful model or generate more responses, until the result passes the example test cases. Moreover, this step can be automated by asking GPT-3.5-Turbo to generate assertion statements from the examples given in the docstr (a simpler task where we can place our bet) and executing the code.</p><h2>Solution</h2><p>Combining these observations, we can design a solution with two intuitive ideas:</p><ul><li>Make use of auto-generated feedback, i.e., code execution results, to filter responses.</li><li>Try inference configurations one by one, until one response can pass the filter.</li></ul><p><img src="img/design.png" alt="Design"/></p><p>This solution works adaptively without knowing or predicting which task fits which configuration. It simply tries multiple configurations one by one, starting from the cheapest configuration. Note that one configuration can generate multiple responses (by setting the inference parameter n larger than 1). And different configurations can use the same model and different inference parameters such as n and temperature. Only one response is returned and evaluated per task.</p><p>An implementation of this solution is provided in <a href="/docs/reference/code_utils#implement">autogen</a>. It uses the following sequence of configurations:</p><ol><li>GPT-3.5-Turbo, n=1, temperature=0</li><li>GPT-3.5-Turbo, n=7, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]</li><li>GPT-4, n=1, temperature=0</li><li>GPT-4, n=2, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]</li><li>GPT-4, n=1, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]</li></ol><h2>Experiment Results</h2><p>The first figure in this blog post shows the success rate and average inference cost of the adaptive solution compared with default GPT-4.
The inference cost includes the cost for generating the assertions in our solution. The generated assertions are not always correct, and programs that pass/fail the generated assertions are not always right/wrong. Despite of that, the adaptive solution can increase the success rate (referred to as pass@1 in the literature) from 68% to 90%, while reducing the cost by 18%.</p><p>Here are a few examples of function definitions which are solved by different configurations in the portfolio.</p><ol><li>Solved by GPT-3.5-Turbo, n=1, temperature=0</li></ol><pre><code class="language-python">def compare(game,guess):
    &quot;&quot;&quot;I think we all remember that feeling when the result of some long-awaited
    event is finally known. The feelings and thoughts you have at that moment are
    definitely worth noting down and comparing.
    Your task is to determine if a person correctly guessed the results of a number of matches.
    You are given two arrays of scores and guesses of equal length, where each index shows a match.
    Return an array of the same length denoting how far off each guess was. If they have guessed correctly,
    the value is 0, and if not, the value is the absolute difference between the guess and the score.


    example:

    compare([1,2,3,4,5,1],[1,2,3,4,2,-2]) -&gt; [0,0,0,0,3,3]
    compare([0,5,0,0,0,4],[4,1,1,0,0,-2]) -&gt; [4,4,1,0,0,6]
    &quot;&quot;&quot;
</code></pre><ol start="2"><li>Solved by GPT-3.5-Turbo, n=7, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]<!-- -->: the <code>vowels_count</code> function presented earlier.</li><li>Solved by GPT-4, n=1, temperature=0:</li></ol><pre><code class="language-python">def string_xor(a: str, b: str) -&gt; str:
    &quot;&quot;&quot; Input are two strings a and b consisting only of 1s and 0s.
    Perform binary XOR on these inputs and return result also as a string.
    &gt;&gt;&gt; string_xor(&#x27;010&#x27;, &#x27;110&#x27;)
    &#x27;100&#x27;
    &quot;&quot;&quot;
</code></pre><ol start="4"><li>Solved by GPT-4, n=2, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]<!-- -->:</li></ol><pre><code class="language-python">def is_palindrome(string: str) -&gt; bool:
    &quot;&quot;&quot; Test if given string is a palindrome &quot;&quot;&quot;
    return string == string[::-1]


def make_palindrome(string: str) -&gt; str:
    &quot;&quot;&quot; Find the shortest palindrome that begins with a supplied string.
    Algorithm idea is simple:
    - Find the longest postfix of supplied string that is a palindrome.
    - Append to the end of the string reverse of a string prefix that comes before the palindromic suffix.
    &gt;&gt;&gt; make_palindrome(&#x27;&#x27;)
    &#x27;&#x27;
    &gt;&gt;&gt; make_palindrome(&#x27;cat&#x27;)
    &#x27;catac&#x27;
    &gt;&gt;&gt; make_palindrome(&#x27;cata&#x27;)
    &#x27;catac&#x27;
    &quot;&quot;&quot;
</code></pre><ol start="5"><li>Solved by GPT-4, n=1, temperature=1, stop=<!-- -->[&quot;\nclass&quot;, &quot;\ndef&quot;, &quot;\nif&quot;, &quot;\nprint&quot;]<!-- -->:</li></ol><pre><code class="language-python">def sort_array(arr):
    &quot;&quot;&quot;
    In this Kata, you have to sort an array of non-negative integers according to
    number of ones in their binary representation in ascending order.
    For similar number of ones, sort based on decimal value.

    It must be implemented like this:
    &gt;&gt;&gt; sort_array([1, 5, 2, 3, 4]) == [1, 2, 3, 4, 5]
    &gt;&gt;&gt; sort_array([-2, -3, -4, -5, -6]) == [-6, -5, -4, -3, -2]
    &gt;&gt;&gt; sort_array([1, 0, 2, 3, 4]) [0, 1, 2, 3, 4]
    &quot;&quot;&quot;
</code></pre><p>The last problem is an example with wrong example test cases in the original definition. It misleads the adaptive solution because a correct implementation is regarded as wrong and more trials are made. The last configuration in the sequence returns the right implementation, even though it does not pass the auto-generated assertions. This example demonstrates that:</p><ul><li>Our adaptive solution has a certain degree of fault tolerance.</li><li>The success rate and inference cost for the adaptive solution can be further improved if correct example test cases are used.</li></ul><p>It is worth noting that the reduced inference cost is the amortized cost over all the tasks. For each individual task, the cost can be either larger or smaller than directly using GPT-4. This is the nature of the adaptive solution: The cost is in general larger for difficult tasks than that for easy tasks.</p><p>An example notebook to run this experiment can be found at: <a href="https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb">https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/research/autogen_code.ipynb</a>. The experiment was run when AutoGen was a subpackage in FLAML.</p><h2>Discussion</h2><p>Our solution is quite simple to implement using a generic interface offered in <a href="/docs/Use-Cases/enhanced_inference#logic-error"><code>autogen</code></a>, yet the result is quite encouraging.</p><p>While the specific way of generating assertions is application-specific, the main ideas are general in LLM operations:</p><ul><li>Generate multiple responses to select - especially useful when selecting a good response is relatively easier than generating a good response at one shot.</li><li>Consider multiple configurations to generate responses - especially useful when:<ul><li>Model and other inference parameter choice affect the utility-cost tradeoff; or</li><li>Different configurations have complementary effect.</li></ul></li></ul><p>A <a href="/blog/2023/04/21/LLM-tuning-math">previous blog post</a> provides evidence that these ideas are relevant in solving math problems too.
<code>autogen</code> uses a technique <a href="https://arxiv.org/abs/2303.04673">EcoOptiGen</a> to support inference parameter tuning and model selection.</p><p>There are many directions of extensions in research and development:</p><ul><li>Generalize the way to provide feedback.</li><li>Automate the process of optimizing the configurations.</li><li>Build adaptive agents for different applications.</li></ul><p><em>Do you find this approach applicable to your use case? Do you have any other challenge to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our <a href="https://discord.gg/pAbnFJrkgZ">Discord</a> server for discussion.</em></p><h2>For Further Reading</h2><ul><li><a href="/docs/Getting-Started">Documentation</a> about <code>autogen</code> and <a href="https://arxiv.org/abs/2303.04673">Research paper</a>.</li><li><a href="/blog/2023/04/21/LLM-tuning-math">Blog post</a> about a related study for math.</li></ul>]]></content>
        <author>
            <name>Chi Wang</name>
            <uri>https://www.linkedin.com/in/chi-wang-49b15b16/</uri>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH]]></title>
        <id>Does Model and Inference Parameter Matter in LLM Applications? - A Case Study for MATH</id>
        <link href="https://microsoft.github.io/autogen/blog/2023/04/21/LLM-tuning-math"/>
        <updated>2023-04-21T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[level 2 algebra]]></summary>
        <content type="html"><![CDATA[<p><img src="img/level2algebra.png" alt="level 2 algebra"/></p><p><strong>TL;DR:</strong></p><ul><li><strong>Just by tuning the inference parameters like model, number of responses, temperature etc. without changing any model weights or prompt, the baseline accuracy of untuned gpt-4 can be improved by 20% in high school math competition problems.</strong></li><li><strong>For easy problems, the tuned gpt-3.5-turbo model vastly outperformed untuned gpt-4 in accuracy (e.g., 90% vs. 70%) and cost efficiency. For hard problems, the tuned gpt-4 is much more accurate (e.g., 35% vs. 20%) and less expensive than untuned gpt-4.</strong></li><li><strong>AutoGen can help with model selection, parameter tuning, and cost-saving in LLM applications.</strong></li></ul><p>Large language models (LLMs) are powerful tools that can generate natural language texts for various applications, such as chatbots, summarization, translation, and more. GPT-4 is currently the state of the art LLM in the world. Is model selection irrelevant? What about inference parameters?</p><p>In this blog post, we will explore how model and inference parameter matter in LLM applications, using a case study for <a href="https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html">MATH</a>, a benchmark for evaluating LLMs on advanced mathematical problem solving. MATH consists of 12K math competition problems from AMC-10, AMC-12 and AIME. Each problem is accompanied by a step-by-step solution.</p><p>We will use AutoGen to automatically find the best model and inference parameter for LLMs on a given task and dataset given an inference budget, using a novel low-cost search &amp; pruning strategy. AutoGen currently supports all the LLMs from OpenAI, such as GPT-3.5 and GPT-4.</p><p>We will use AutoGen to perform model selection and inference parameter tuning. Then we compare the performance and inference cost on solving algebra problems with the untuned gpt-4. We will also analyze how different difficulty levels affect the results.</p><h2>Experiment Setup</h2><p>We use AutoGen to select between the following models with a target inference budget $0.02 per instance:</p><ul><li>gpt-3.5-turbo, a relatively cheap model that powers the popular ChatGPT app</li><li>gpt-4, the state of the art LLM that costs more than 10 times of gpt-3.5-turbo</li></ul><p>We adapt the models using 20 examples in the train set, using the problem statement as the input and generating the solution as the output. We use the following inference parameters:</p><ul><li>temperature: The parameter that controls the randomness of the output text. A higher temperature means more diversity but less coherence. We search for the optimal temperature in the range of <!-- -->[0, 1]<!-- -->.</li><li>top_p: The parameter that controls the probability mass of the output tokens. Only tokens with a cumulative probability less than or equal to top-p are considered. A lower top-p means more diversity but less coherence. We search for the optimal top-p in the range of <!-- -->[0, 1]<!-- -->.</li><li>max_tokens: The maximum number of tokens that can be generated for each output. We search for the optimal max length in the range of <!-- -->[50, 1000]<!-- -->.</li><li>n: The number of responses to generate. We search for the optimal n in the range of <!-- -->[1, 100]<!-- -->.</li><li>prompt: We use the template: &quot;{problem} Solve the problem carefully. Simplify your answer as much as possible. Put the final answer in <!-- -->\<!-- -->boxed{{}}.&quot; where {problem} will be replaced by the math problem instance.</li></ul><p>In this experiment, when n &gt; 1, we find the answer with highest votes among all the responses and then select it as the final answer to compare with the ground truth. For example, if n = 5 and 3 of the responses contain a final answer 301 while 2 of the responses contain a final answer 159, we choose 301 as the final answer. This can help with resolving potential errors due to randomness. We use the average accuracy and average inference cost as the metric to evaluate the performance over a dataset. The inference cost of a particular instance is measured by the price per 1K tokens and the number of tokens consumed.</p><h2>Experiment Results</h2><p>The first figure in this blog post shows the average accuracy and average inference cost of each configuration on the level 2 Algebra test set.</p><p>Surprisingly, the tuned gpt-3.5-turbo model is selected as a better model and it vastly outperforms untuned gpt-4 in accuracy (92% vs. 70%) with equal or 2.5 times higher inference budget.
The same observation can be obtained on the level 3 Algebra test set.</p><p><img src="img/level3algebra.png" alt="level 3 algebra"/></p><p>However, the selected model changes on level 4 Algebra.</p><p><img src="img/level4algebra.png" alt="level 4 algebra"/></p><p>This time gpt-4 is selected as the best model. The tuned gpt-4 achieves much higher accuracy (56% vs. 44%) and lower cost than the untuned gpt-4.
On level 5 the result is similar.</p><p><img src="img/level5algebra.png" alt="level 5 algebra"/></p><p>We can see that AutoGen has found different optimal model and inference parameters for each subset of a particular level, which shows that these parameters matter in cost-sensitive LLM applications and need to be carefully tuned or adapted.</p><p>An example notebook to run these experiments can be found at: <a href="https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb">https://github.com/microsoft/FLAML/blob/v1.2.1/notebook/autogen_chatgpt.ipynb</a>. The experiments were run when AutoGen was a subpackage in FLAML.</p><h2>Analysis and Discussion</h2><p>While gpt-3.5-turbo demonstrates competitive accuracy with voted answers in relatively easy algebra problems under the same inference budget, gpt-4 is a better choice for the most difficult problems. In general, through parameter tuning and model selection, we can identify the opportunity to save the expensive model for more challenging tasks, and improve the overall effectiveness of a budget-constrained system.</p><p>There are many other alternative ways of solving math problems, which we have not covered in this blog post. When there are choices beyond the inference parameters, they can be generally tuned via <a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function"><code>flaml.tune</code></a>.</p><p>The need for model selection, parameter tuning and cost saving is not specific to the math problems. The <a href="https://github.com/Significant-Gravitas/Auto-GPT">Auto-GPT</a> project is an example where high cost can easily prevent a generic complex task to be accomplished as it needs many LLM inference calls.</p><h2>For Further Reading</h2><ul><li><a href="https://arxiv.org/abs/2303.04673">Research paper about the tuning technique</a></li><li><a href="/docs/Use-Cases/enhanced_inference">Documentation about inference tuning</a></li></ul><p><em>Do you have any experience to share about LLM applications? Do you like to see more support or research of LLM optimization or automation? Please join our <a href="https://discord.gg/pAbnFJrkgZ">Discord</a> server for discussion.</em></p>]]></content>
        <author>
            <name>Chi Wang</name>
            <uri>https://www.linkedin.com/in/chi-wang-49b15b16/</uri>
        </author>
    </entry>
</feed>